1. 开篇词：数据分析思维才是你的核心竞争力！
2. [数据分析最爱用的估算法：费米估计](#数据分析最爱用的估算法：费米估计)
3. [数据分析中违背常理的悖论：辛普森悖论](#数据分析中违背常理的悖论：辛普森悖论)



## 开篇词：数据分析思维才是你的核心竞争力！

随着大数据技术在各行各业应用的越来越广，数据驱动智能产品和精细化运营已经成为企业经营的制胜法宝，相应地，数据分析师这个岗位也越来越受到关注。后互联网时代，企业的数字化转型已经成为必由之路，企业由数据驱动，工作需要数据思维，结论先行，数据跟上，人人都是数据分析师！

**数据分析师已成为行业标配**

当餐馆扫码下单、机器人送餐、刷脸支付完成时，当机场、火车站安检开了人脸识别通道，当 ETC 已经在取代人工岗亭，当智能客服取代了人工客服，甚至连外卖小哥也面临无人配送的挑战时……

信息革命已经来了，数据驱动的产品越来越多智能化，智能设备逐步取代人类，成为我们社会的一个组成部分。在这样的一个大趋势下，未来每一个人的工作都会与智能化的设备有交集，而这所谓的“交集”正是由数据组成的，数据成为了人与设备之间的沟通的桥梁。而各行业的互联网化也会让业务数据开始增多，更多的业务决策将依赖于数据。

而作为与数据打交道最密切的职业——数据分析师，也将迎来了黄金就业期。据艾瑞研究统计，在过去的两年中，由于各行各业新聘用了 80 万名数据科学家，数据科学岗位的短缺问题已大大缓解；但是今天市场上仍然有成千上万的空缺职位，其中大部分在美国和中国。

通过搜索 BOSS 直聘和领英，发现其上面有上有 10 万 + 个数据分析师职位空缺，其中绝大部分是互联网行业的需求。值得注意的是，虽然国内现有很多数据分析师员工，但其数量占比依旧很少，职位空缺却占到了市场的 50% 之多。

数据分析师的岗位，越来越受到大家的关注。而且越来越多的小伙伴也转行做数据分析，因为大家不仅看到的是未来数据分析的发展前景，而且数据分析师的薪资待遇也很不错，未来还存在巨大的潜力等待更多人来挖掘。

**数据分析师的求职困惑**

企业大量需求的背后，对数据分析人才的要求也在不断提高，根据掌握技能和解决问题的能力，我们把数据分析师大体分为以下 3 类：

- 第一类：初级，只会被动的取数。没办法解决业务的问题，业务部门缺什么数据，我就取什么数据……常被调侃叫”茶树菇/表哥/表姐“。
- 第二类：中级，解决具体问题。采用由上至下的思维方式来分析，通过做用户画像—寻找差异-差异量化成指标—问题假设—改进方案—验证。
- 第三类：高级，指导业务。除了数据分析外，跟一线的聊，跟不同部门的领导聊，分析到底这个是不是数据问题？帮助大家梳理清楚有几条路，要怎么走这条路，能参与公司决策。

对于求职的数据分析师来说，明明按照招聘要求认真写的简历，精通 SQL、Python，学历、技能和工作年限等都比较吻合，眼看着非常和自己匹配的岗位，可投递过去要么石沉大海要么一面挂无缘二面，这是为什么呢？

我咨询了几个数据分析招聘负责人，得到的回答基本一致，工具的使用是他们最基本的要求，更看重的是候选人的分析思维和综合素质。

每当面试受挫，我相信很多人的行动就是开始找网课视频，然后学习，花费时间多，结果也不是很好。因为网课视频都是教你工具怎么使用，但没办法帮你摆脱”人肉取数机“，真正能从业务思维角度带你做数据分析的，少之又少。

**专栏设计**

通过 NLP 技术，对上千篇数据分析面经进行信息抽取和统计提炼出来的高频考点。一般情况下，普通面试者可以通过网络阅读到别人写的面经，但是这些面经缺少准确的知识点拆解和完整的答题思路，如果面试者花费大量的时间和精力去检索答案，显然对面试是得不偿失的。

面试题虽然千人千面，无数种问法，其实万变不离其宗，如果掌握了数据分析的核心模型和方法论，遇到问题就会回答的有理有据，让面试官眼前一亮。

因此，本专栏通过大数据分析方法，直接切中数据分析师面试的高频考点，通过理论模型 + 企业真题的学习实践，各个击破知识点，扫清面试路上的拦路虎，助力求职者早日拿到心仪的 Offer。

本专栏包括：逻辑推理篇、思维模型篇、指标搭建篇、业务专题篇和 SQL 实战篇共 5 个篇章：

通过这些内容的学习，相信你可以在面试中回答的让面试官眼前一亮，掌握了这些内容，更可以提升你的业务分析思维，让你终身受用。

**作者寄语**

互联网行业还在蓬勃发展，未来数据分析师将扮演越来越重要的角色，越来越多的公司和业务决策需要数据分析的支持，数据分析师也将成为一个具有挑战的职位，除了掌握核心技能，对于分析思维和行业知识的了解将成为大多数职业发展路上的拦路虎。

对于任何行业、任何一个人来说，未来与数据打交道的频率和机会只会越来越多，参与的越来越深，而数据分析思维可以说是未来任何工作中最核心的竞争力之一。

率和机会只会越来越多，参与的越来越深，而数据分析思维可以说是未来任何工作中最核心的竞争力之一。

本专栏将对数据分析思维从理论 + 实战方面进行讲述，助你快速拿到 Offer，在实际工作中，积累更多的业务经验和行业知识。



## 数据分析最爱用的估算法：费米估计

在科学研究中，有这样一类估算问题，初次接触会觉得已知条件太少，无法得出答案，但如果对分析对象进行变通替换，问题就会迎刃而解，这就是费米问题。它可以用来对给定有限信息的问题做出清晰地验证估算。

#### 费米估计的起源

有这样的一个故事：

> 1945 年世界上第一颗原子弹爆炸。费米在感觉到震波的同时，把举过头顶的笔记本碎纸屑松开，碎纸屑落在身后 2.5 米的距离，通过心算后，得出结论原子弹的能量相当于 10000t TNT 的量，后来一些高科技仪器证明了费米的估算是正确的。

同样的故事还有：

> 地球的周长是多少？

使用费米估计的解决方法是：已知纽约到洛杉矶 3000 英里，时差 3 小时，而一天即地球自转一周的时间为 24 小时，即 3 小时的 8 倍。所以，地球的周长就是 3000 乘以 8，等于 24000 英里。与精确值的 24902.45 英里相比，误差不到 4%。

> 芝加哥有多少钢琴调音师？

使用费米估计的解决方法是： 如果芝加哥居民 300 万，平均每户 4 人，拥有钢琴的家庭占 1/3，则全市有 250000 架钢琴。如果一架钢琴每 5 年调音一次，则全市每年有 50000 架钢琴要调音。如果一个调音师一天调 4 架钢琴，一年工作 250 天，那么，芝加哥市大约有 50 个调音师。

通过上面的例子，可以看出，直接通过非缜密计算可以快速把一个给定有限信息的费米难题转化成简单问题，进而快速得到解决，费米问题也因此受到人们的重视。

#### 费米估计的原理

通过上面的案例，费米估计指的是解决少量信息的复杂估算问题，将复杂的问题拆解成常识性和已知的小问题，进而进行计算得到结果。所以解决问题的关键，主要有 2 个方面。

**问题拆解**

将未知的数逐步拆解成已知的部分，从而将一个未知结果的问题逐步变得清晰。问题拆解要按照一定的原则进行，比如使用 MECE 原则，做到不重不漏，复杂问题层层拆解为简单的子问题，从而解决问题。

在麦肯锡分析思维中，拆解问题的这种方法叫做议题树（也叫逻辑树或 MECE 原则），图下所示：

![x](D:\WorkingDir\Office\Resources\da0001.png)

使用议题树对理清复杂问题的结构非常有帮助。

**实际的常识数据准确**

当我们拆解到实际子问题的时候，要确保实际常识代表的数据是有生活经验的支撑，而非凭空捏造的。

所以，我们可以总结出费米估计的一个具体步骤：

1. 首先，明确问题；
2. 按照 MECE 原则进行问题拆解；
3. 明确常识性已知数据；
4. 设计计算公式；
5. 计算，得出结果。

#### 费米估计解决方法：Top-Down&Bottom-Up 法则

费米问题的解决，不仅需要有很强的思维逻辑，同时还需要广泛的常识性知识和准确数据的支持，非常锻炼一个人多方位的思考分析能力。在生活中实际用途也很多，比如预测行业趋势、估算市场份额、评估活动结果等。

解决费米问题的方法很多，比较典型的是 Top-Down&Bottom-Up 法则。

Top-Down&Bottom-Up 法则的中心思想是：

首先，从 2 个方面展开拆解问题。一是自上而下，也就是纵向从宏观到微观直推；二是自下而上，横向从局部到全部反推。其次，对自上而下和自下而上的结果要进行对比，如果两者相减的绝对值在一个可接受的范围，那结果基本可以相信。最后，需要对可能产生误差的点进行补充，保证结果更加精确。

通俗地来说，自上而下和自下而上就是估算结果的两个边界，大的为上边界，小的为下边界，而最终的结果就落在两个边界里面了。

假设 f(x) 是我们求解的结果，F(x) 是下边界值，G(x) 是上边界值，则 f(x) 我们可以类似想到用夹逼准则的方式，看成是 F(x) <= f(x) <= G(x)，f(x) 就是可信的。

从数学的角度，我们再来看一个问题，古代三大几何难题之一的化圆为方。如今计算圆的面积，我们直接套用公式即可轻易得到，可在过去，人们是却花费了大量的时间和精力。

圆面积公式的常规推导思路是：先把一个圆平均分成若干份，然后将其拼成近似的长方形，最后根据长方形与圆的关系推导出圆的面积公式。当时人们认为既然正方形的面积容易求，只需要想办法做出一个面积恰好等于圆面积的正方形。但是怎样才能做出这样的正方形又成为了另外一个难题。

这个起源于古希腊的几何作图题，在 2000 多年里，不知难倒了多少能人，直到 19 世纪，人们才证明了这个几何题，古代人的尺规作图法是解决不了该问题的。

关于圆面积，古代数学家的贡献如下：

> 我国古代的数学家祖冲之，从圆内接正六边形入手，让边数成倍增加，用圆内接正多边形的面积去逼近圆面积。古希腊的数学家，从圆内接正多边形和外切正多边形同时入手，不断增加它们的边数，从里外两个方面去逼近圆面积。
>
> 古印度的数学家，采用类似切西瓜的办法，把圆切成许多小瓣，再把这些小瓣对接成一个长方形，用长方形的面积去代替圆面积。16 世纪天文学家开普勒，把圆分割成许多小扇形，圆面积等于无穷多个小扇形面积的和，各段小弧相加就是圆的周长 2πR，最后得出我们今天计算圆面积的公式。

通过今天的计算可以得到：在一个圆里画一个最大的正方形，正方形占圆面积的约 63.7%，在一个圆外画一个最小的正方形，正方形面积是圆形面积的 157%。

看看，古人计算圆面积的思想和费米估计的思想是不是有异曲同工之妙呢？

**案例分析**

在互联网领域的工作面试中，如数据分析、产品、市场和咨询等岗位，费米问题经常被考到，题目往往能间接反映出候选人的综合素质。

下面，我们基于一个外卖业务，做一个分析。在北京地区，需要多少骑手才能满足用户的外卖需求呢？

**1. 明确问题**

对于一家外卖平台，如果能提前估算出平台所需要的骑手人数，首先对人力资源方面会节省非常大的成本，其次可以根据商业模式提前计算业务数据，比如定价、订单分发、补贴、骑手支出等。给业务提供许多决策依据。

**2. 问题拆解**

北京地区每天有多少单外卖需求？1 个骑手 1 天能够完成多少单任务？1 个骑手完成一单的花费时间是多少？

每单花费时间：前往商家时间 + 排队等待时间 + 从商家到目标地点时间 + 等待用户时间。

**3. 明确常识性数据**

根据 2019 年公布的数据，北京人口总数 2153.6 万人，网民渗透率 90% 以上，对于外卖的目标用户主要锁定在 20~30 岁易接受新鲜事物和消费能力强的年轻人，这部分人在人群中占比 45%。

- 目标人群规模：`2153.6*90%*45*=872.2 万人`
- 需求频度：每人平均 5 天 1 次
- 骑手每天工作有效时间：10h，吃饭休息除外
- 商家距离：正常是 3 公里以内，我们取 3 公里为基数
- 骑手速度：根据政策来说，电动车限速在 25km/h
- 排队等待时间为：10~20 分钟，我们取 15 份，即 0.25h
- 商家到目的地距离：正常应该是 3 公里以内，3~5 公里比较普遍，我们取 4 公里为基数
- 用户等待时间：由于限时配送，我们认为都是按时或提起配送，基本无超时订单，所以用户等待时间为 0h

**4. 设计计算公式**

> 所需的骑手总数 = 每天订单总数/每人每天可配送的订单数

则可以确定以下数据：

> 每天的总订单数 = 目标用户数/点单频度 = 872.2/5 = 174.44 万单
>
> 每人每天可配送的订单数 = 每天工作时间/完成一个订单需要的时间
>
> = 每天工作时间/（骑手到商家的时间+排队等待时间+配送时间 + 用户等待时间）
>
> = 每天工作时间/（商家距离/骑手速度+排队等待时间+目的地距离/骑手速度 + 用户等待时间 ）
>
> = 10/（3/25 + 0.25 + 4/25 + 0）
>
> = 18.9 单
>
> 所需的骑手总数 = 每天订单总数/每人每天可配送的订单数 = 92296 人

所以，最终得出的结论，北京应该有大约 9 万外卖骑手。

从上面的分析，我们也看到一个现实，每个骑手每天工作 10 小时，平均可以配送 19 单，新手的话可能更少，所以非常辛苦，这也能够激发大家的同理心，所以平时点外卖对于超时的订单要包容下。

除了上面的问题，类似这样奇怪的问题还有：

- 一个正常成年人有多少根头发？
- 在没有其他公开资料，你所在城市有多少个加油站？
- 地铁口的煎饼摊子一年能卖出多少个煎饼？
- 一辆公交车里能装下多少个乒乓球？
- 一个人一生会长出多长的手指甲？
- 估算你所在城市有多少家奶茶店？

总结：费米问题是现实世界最好的估算法，能够把限定信息、复杂困难的问题通过分解成常识性、可解决的小问题处理，所谓大事化小小事化了，这种思维方式非常实用，通过本篇，希望可以帮助你解决更多问题。



## 数据分析中违背常理的悖论：辛普森悖论

在现实生活中，我们常常会遇到这样一种现象，当尝试研究两个变量是否具有相关性的时候，会分别对此进行分组研究。

然而，在分组比较中都显示非常有优势的一方，在总评时却成了失势的一方。直到 1951 年，英国统计学家 E.H.辛普森发表论文对此现象做了描述解释，后来人们就以他的名字命名该现象，即辛普森悖论。

思考下，辛普森悖论为什么成立？

#### 辛普森悖论的原理

下面给出辛普森悖论的数学原理：

![x](D:\WorkingDir\Office\Resources\da0002.png)

从数学表达式上，我们可以看出，对 a、b、c、d 四个变量，分成 1 组和 2 组，在 1 组比率占优势的情况下，总体占优势却不成立。

看一个例子：抖音 6 月与 7 月活跃人群得活跃时长对比，发现男性活跃时长上升，女性也上升，但是整体上 7 月活跃时长比 6 月降低是什么原因？

关于避免辛普森悖论的出现，目前比较流行的一种做法，就是需要斟酌个别分组的权重，以一定的系数去消除以分组资料基数差异所造成的影响，同时必须了解该情境是否存在其他潜在因素，需要进行综合性考虑。在实际中斟酌权重和判断其他因素，大多数还是更多依赖经验。

**贝叶斯定理**
$$
P(A|B) = \frac{P(B|A)}{P(B)}P(A)
$$
先看一个例子，小易生病去医院，做完检查结果呈阳性，医生告诉他可能是患上了 XX 疾病，吓得他惊慌失措，冷静之余，他赶忙到网上查询资料，网上说检查总是有误差的，这种检查有“百分之一的假阳性率和百分之一的假阴性率”。

这句话的意思是说，在得病的人中做实验，有 1% 的人是假阳性，99% 的人是真阳性。而在未得病的人中做实验，有 1% 的人是假阴性，99% 的人是真阴性。

于是，小易根据这种解释，估计他自己得了 XX 疾病的可能性（即概率）为 99%。可是，医生却告诉他，他被感染的概率只有 0.09 左右。这是怎么回事呢？

医生说：你忘了一件事，XX 病在人口中的得病基本比例（1/1000）这个事实。

医生给出计算方法：因为测试的误报率是 1%，1000 个人将有 10 个被报为“假阳性”，而根据 X 病在人口中的比例（1/1000=0.1%），真阳性只有 1 个。所以，大约 11 个测试为阳性的人中只有一个是真阳性（有病）的，因此，小易被感染的几率是大约 1/11，即 0.09（9%）。

从贝叶斯定理的原理，解释小易被感染的几率就计较容易了。

- A：普通人群中的小易感染 XX 病
- B：阳性结果
- P(A)：普通人群中感染 X 病的概率
- P(B|A)：阳性结果的概率
- P(A|B)：有了阳性结果条件下，小易感染 XX 病的概率
- P(B)：结果为阳性的总可能性 = 检查阳性中的真阳性 + 检查阴性中的真阳性

![x](D:\WorkingDir\Office\Resources\da0003.png)

类似的悖论，还有罗杰斯现象、伯克森悖论、生日悖论等。

总结：辛普森悖论让我们明白，在因果关系里，量与质是不等价的，但是量比质更容易测量，所以人们总是习惯用量来评定好坏，而该数据却不是重要的。辛普森悖论带给我们的另外一个启示是：如果我们在人生的抉择上选择了一条比较难走的路，就得具备可能不被赏识、怀才不遇的心理准备。

马斯洛认为，人的需求由生理的需要、安全的需要、归属与爱的需要、尊重的需要和自我实现的需要五个等级构成。

从不同岗位的数据分析师和数据相关岗位工作内容来看，目的有主要以下几种：

- 收集、整理和标准化数据，对数据进行存档管理，更专业的比如做数据仓库，大数据平台等，为数据商业化应用做基础建设；
- 不同行业和专题的数据抽取、分析和可视化展现，目的是辅助管理和决策，提供可优化策略，进行商业智能落地；
- 根据已有数据进行分类、预测和聚类，通常使用算法和统计技术进行挖掘建模，使场景模型化，进行精准营销、推荐和其他商业活动。

数据分析不管是做分析、辅助决策亦或是商业智能，最终的目的，都在寻找商业上的最优解决方案，通过这样的最优方案，可以节省资源、降低成本、精准营销、提升利润等等，其本质，就是要提升企业的竞争力，让企业获得更大的利润收入。

按照大多数企业的发展规律，假设企业发展阶段按照融资轮次可分成：天使轮、A 轮、B 轮、C 轮、D 轮等（这个融资轮次不是固定的，这里只是假设举例）。

天使轮：在这个阶段，创业公司刚刚起步，产品进入开发或者冷启动阶段，这个时候，企业要进行数据分析，可现实问题基本没有数据的，那没有数据就不做分析了？肯定不行，访谈法和问卷调查法是这个阶段主流的收集数据和调研问题的解决方法，靠这种方法，面对面沟通，可以对产品定位、用户痛点等有一个直接实质性的把控，对于种子用户的积累起到非常精准的拉新。所以，对于大多数成功的产品，种子用户都是精准用户。

A 轮：在这个阶段，产品解决用户痛点的需求其实已经被验证了，产品已经上线，这个阶段的产品工作重心是要快速打入市场，获取用户和流量，甚至通过补贴等手段进行拉新，抢占市场份额，成功的产品该阶段后期用户将进入指数级增长，所以，该阶段数据分析的核心围绕拉新展开的，除了拉新，由于平台补贴，识别羊毛党也是挺重要的一个点。

B 轮：在这个阶段，如果发展的不错，用户已经初据规模，这个时候用户需求和产品之间又会出现冲突，而且企业要开始考虑盈利的问题了。所以该阶段，既要不断拉新，还要快速迭代产品，还要重视用户活动度、留存和转化问题，产品基本已经可以盈利和生存下去。数据分析在这个阶段做事其实挺难的，虽然用户初据规模，但是用户行为、转化等数据其实并不多，用户拉新、留存分析还相对容易点，但是用户偏好习惯、消费规律等数据还是不够，深入挖掘分析其实有问题，所以需要不断的运营手段来促活，产生更多的数据来进行挖掘建模。所以一般互联网公司，B 轮的阶段大多数才开始搭建大数据平台和数据收仓，数据分析也是提取和临时需求较多，而挖掘建模、机器学习做的并不多，最多就是做做用户画像和推荐系统。

C 轮和 D 轮：在这个阶段，产品已经进入市场且具有竞争力，用户规模很大，产品更加稳定和成熟，整个数据链路基本形成闭环，但是用户获客成本越来越高，这个时候是最能体现数据分析师价值的时候，数据分析的工作也开始更加细化和专一，典型的数据分析包括渠道分析、用户增长分析、用户行为分析、产品活动分析和经营分析等，而这一切都会围绕着企业总收入 GMV 或者投资回报率 ROI 来进行分析，说直接点，就是这个阶段，数据分析的一切都会围绕着产品变现进行，要不断扩大收入和利润，让整个产品生态进入良性循环，为最后的 IPO 做准备。

综合上面，可以看出来，企业在不同发展时期，其数据分析需求是不一样的，尤其短中期的分析目标要随着企业战略和业务方向随时调整，且临时性需求比较多。

之前面试的时候，有一位面试官问我，怎么看待数据分析，数据分析的本质是什么？我吧啦吧啦说了一大堆。

在反问环节，我问面试官认为的数据分析的本质是什么？面试官说，他认为数据分析的本质就是拆指标，拆的越细越准确。

不可否认，我是比较赞同面试官这个解释的，很多时候，我们做工作的思路就是运用 MECE 原则进行指标不同维度的拆解。但是这是一种狭义上的理解，如果业务早期的时候，没有数据，靠外部数据又不行的情况下，你怎么拆？没有数据还是要回归原始和潜在用户面对面的交流，这才是最有效的。

所以数据分析它不是独立存在的，必须贴近业务和产品，数据取之于产品，还要用之于产品，数据分析本身却不产生数据。

总结：数据分析的本质，狭义上来说就是拆解指标，但这个本质我认为虽然终极目标可能一样，但是在人类发展或者企业发展的不同阶段，目的是不一样的，着重解决的问题也是不一样的，会随着外部事物或者产品发展而顺势变化。也就是说，数据分析始终贯彻执行在业务和产品上，**数据取之于产品，还要用之于产品**，数据分析本身却不产生数据，只是提供一种思维方法论，一种工具而已。

**解决问题的思维方式造就了不同的结局，在工作中，除了努力，有一些方法论是必须要掌握的，我们不能用战术上的勤奋掩盖战略上的懒惰**。

下面分享两个原则，它俩不仅局限在数据分析，在解决其他现实、管理、创业问题时同样适用。

先说说这两个原则是什么呢？**帕累托原则**和 **MECE 原则**。帕累托原则从宏观的角度考虑，把握问题的重点与核心部分，而 MECE 原则偏向从微观的角度，层层递进，做到综合全面。

什么是帕累托原则呢？其实就是我们耳熟能详的二八原则，也叫也叫巴莱特定律、朱伦法则（Juran's Principle）、关键少数法则（Vital Few Rule）、不重要多数法则（Trivial Many Rule）最省力的法则、不平衡原则等。

当我们面对一长系列的数据或者问题是，如果一个个穷举解决，要花费很长的时间和精力，让人心存畏惧觉得不能完成，如果运用帕累托原则，我们就可以快速定位主次因素，然后决定解决问题的优先级，先解决主要因素，再解决次要问题。

通过一些学者的研究，对于二八原则，基本有这样一些被认可的结论：

- 80% 的销售额来自 20% 的渠道；
- 80% 的订单来自 20% 的顾客；
- 80% 的营业利润来自 20% 的成交。

举例来说， 根据某电商广告投放来说 ，在业务发展某一阶段，老板和领导层定的 KPI 主要是 GMV，如果某天的产品 GMV 波动很大，上升了当然的好事，可如果下降超出预期，该怎么办呢？

首先，我们应该抓住主要的关键因素，看头部渠道如淘宝、支付宝、京东具体的变化，因为他们贡献的 GMV 的变化更能影响整体 GMV 的变化。

做数据分析时，我们最容易犯的错误就是被平均数迷惑，**聪明的人都会放弃平均，古有楚汉之争，今有划江而治， 蛋糕是无法被平分的，唯有做大做强**。

什么是 MECE（Mutually Exclusive Collectively Exhaustive）原则？其含义指拆解问题时要做到相互独立，完全穷尽。

在什么情况下，我们会考虑使用 MECE 原则呢？往往是遇到比较复杂的问题，或者是面对金字塔顶端的问题或者指标时，为了避免以偏概全和重叠导致无法梳理清楚真正的问题原因时，我们会选择使用 MECE 原则，进行问题拆解。

还是以上面的例子，假如某天的产品 GMV 波动很大，下降超出预期，我们怎么用 MECE 来分析呢？

第一步，确定当下的问题，GMV 下降了，且超出预期，不是数据问题，接下来确定我们的目的是要找到问题原因，并且给出优化建议；

第二步，找到符合 MECE 的切入点，进行分类。这里我们使用公式法进行问题拆解。

```markdown
GMV 
= 订单数 × 客单价
=（渠道 A 订单数 × 转化率 + 渠道 B 订单数 × 转化率 +...+ 渠道 N 订单数 × 转化率）× 客单价
=（渠道 A 流量 × 转化率 + 渠道 B 流量 × 转化率 +...+ 渠道 N 流量 × 转化率）× 客单价
= ......
```

比如拆解到渠道上面，如果有很多渠道怎么办，这个时候，通过二八原则来定位下，根据渠道贡献 GMV 占比，找到最主要影响 GMV 下降的渠道，假如通过分析发现果然主要由渠道 A 引起的，那我们可以继续从渠道 A 进行分析了。

假设渠道 A 用户下单转化流程是：浏览访问—加购物车—下单—支付，采用漏斗分析法，从上面的转化漏斗，我们发现用户支付的转化率比较低，从下单到支付成功，只有 28.2% 的转化率，这有点不正常，是什么原因造成了支付转化率较低呢？

![x](D:\WorkingDir\Office\Resources\da0004.png)

到目前为止，我们定位到了更具体的原因，大盘 GMV 下降的原因是，渠道 A 引起的，主要是渠道 A 支付转化率降低引起的，那是什么原因引起渠道 A 转化率降低呢？

这个时候继续分析，采用二分法，从内部和外部环境来看。

如果从外部分析的话，使用 PEST 模型，从政治、经济、社会和技术方面考虑。

如果从内部分析的话，作为分析师，我们有必要去体验下产品下单到支付路径，有没有发生什么原因和变化，其次，基于业务理解的理解，不看不知道，一看吓一跳，原因找到了，由于做活动，商品介绍的海报中指定满减的活动，让用户看起来买的很划算，但是在真实支付的时候，并没有满减，而是提示优惠额度已经用完，提示用户是否继续支付，所以很多用户就放弃了。

所以，问题的原因在于，海报中介绍的满减活动，由于名额问题，并没有执行好，再深刻可解释为存在欺骗用户和引诱下单的行为。

所以，针对该问题，我们提出如下建议：

- 尽快恢复满减额度，确保知行合一；
- 或者撤销活动海报的满减优惠；
- 如果满足条件，给未支付的用户发短信通知满减消息的真实性，引导其回来支付等。

以上，通过很简单的实例来介绍数据分析的两大原则。