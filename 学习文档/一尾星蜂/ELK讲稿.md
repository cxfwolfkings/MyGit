#### 基础简介

准备工作：

```sh
mkdir -p /data/es/1/config /data/es/1/data \
         /data/es/2/config /data/es/2/data
chmod 777 /data/es/ -R
```

配置文件：

```yml
# node1
cluster.name: leadchina
node.name: es-node1
network.bind_host: 0.0.0.0
network.publish_host: 172.18.0.8
http.port: 9200
transport.tcp.port: 9300
http.cors.enabled: true
http.cors.allow-origin: "*"
node.master: true 
node.data: true  
discovery.zen.ping.unicast.hosts: ["172.18.0.8:9300","172.18.0.9:9300"]
discovery.zen.minimum_master_nodes: 2

# node2
cluster.name: leadchina
node.name: es-node2
network.bind_host: 0.0.0.0
network.publish_host: 172.18.0.9
http.port: 9200
transport.tcp.port: 9300
http.cors.enabled: true
http.cors.allow-origin: "*"
node.master: true 
node.data: true  
discovery.zen.ping.unicast.hosts: ["172.18.0.8:9300","172.18.0.9:9300"]
discovery.zen.minimum_master_nodes: 2
```

启动：

```sh
# node
docker run -d \
           -p 9203:9200 -p 9303:9300 \
           -e ES_JAVA_POTS="-Xms128m -Xmx128m" \
           -e "discovery.type=single-node" \
           -v /data/es/1/data:/usr/share/elasticsearch/data \
           --name es-node-single \
           --net exceptionless-500_default \
           elasticsearch:7.8.0

# node1
docker run -d \
           -p 9201:9200 -p 9301:9300 \
           -e ES_JAVA_POTS="-Xms128m -Xmx128m" \
           -v /data/es/1/config/a.yml:/usr/share/elasticsearch/config/elasticsearch.yml \
           -v /data/es/1/data:/usr/share/elasticsearch/data \
           --name es-node1 \
           --net exceptionless-500_default \
           elasticsearch:7.8.0

# node2
docker run -d \
           -p 9202:9200 -p 9302:9300 \
           -e ES_JAVA_POTS="-Xms128m -Xmx128m" \
           -v /data/es/2/config/a.yml:/usr/share/elasticsearch/config/elasticsearch.yml \
           -v /data/es/2/data:/usr/share/elasticsearch/data \
           --name es-node2 \
           --net exceptionless-500_default \
           elasticsearch:7.8.0
```

**为什么要用ES？**

我们的世界已被数据淹没。不幸的是，大部分数据库在从数据中提取可用知识时出乎意料的低效，不能满足我们的需求。在一般数据库中，我们可以通过时间戳或精确值进行过滤，但是高效地进行全文检索、同义词处理、通过相关性给文档评分，从同样的数据中生成分析与聚合数据，最重要的是，不经过大型批处理任务就实时做到这些操作？ES能做到，这就是它脱颖而出的地方：Elasticsearch 鼓励我们去探索与利用数据，不因为查询数据太困难，就让它们烂在数据仓库里。

**起源**



**流行度**



**定义**

ES是一个开源的搜索引擎，建立在一个全文搜索引擎库 [Apache Lucene™](https://lucene.apache.org/core/) 基础之上。 Lucene 可以说是当下最先进、高性能、全功能的搜索引擎库—无论是开源还是私有。

ES 使用 Java 编写，内部使用 Lucene 做索引与搜索，它的目的是使全文检索变得简单，所以通过封装隐藏了 Lucene 的复杂性，取而代之的是一套简单一致的 RESTful API。

当然ES不仅仅只是一个全文搜索引擎。 它可以被下面这样准确的形容：

- 一个分布式的实时文档存储，每个字段可以被索引与搜索
- 一个分布式实时分析搜索引擎
- 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据

ES 将所有的功能打包成单独的服务，这样我们可以通过程序与它提供的简单的 RESTful API 进行通信，可以使用自己喜欢的编程语言充当 web 客户端。

**ES核心概念**

1）Cluster：集群

ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。

2）Node：节点

形成集群的每个服务器称为节点。

**3）Index：索引**

在 ES 中，索引是一组文档的集合。索引的作用相当于图书的目录，可以根据目录中的页码快速找到所需的内容。当表中有大量记录时，若要对表进行查询，第一种搜索信息方式是全表搜索，是将所有记录一一取出，和查询条件进行一一对比，然后返回满足条件的记录，这样做会消耗大量数据库系统时间，并造成大量磁盘I/O操作；第二种就是在表中建立索引，然后在索引中找到符合查询条件的索引值，最后通过保存在索引中的ROWID（相当于页码）快速找到表中对应的记录。

4）Shard：分片

当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片，每个分片放到不同的服务器上。

当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。

需要注意：在创建索引的时候就确定好主分片的数量，并且永远不能改变这个数量。

![x](http://121.196.182.26:6100/public/images/tempsnip.png)

比如上图所示，开始设置为5个分片，在单个节点上，后来扩容到5个节点，每个节点有一个分片。如果继续扩容，是不能自动切分进行数据迁移的。官方文档的说法是分片切分成本和重新索引的成本差不多，所以建议干脆通过接口重新索引。

路由一个文档到一个分片：

当索引一个文档的时候，文档会被存储到一个主分片中。Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？

首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：

shard = hash(routing) % number_of_primary_shards

routing 是一个可变值，唯一不可重复，默认是文档的 _id ，也可以设置成一个自定义的值。routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards（主分片的数量）后得到余数。这个分布在 0 到 number_of_primary_shards - 1 之间的余数，就是我们所寻求的文档所在分片的位置。

这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。

所有的文档 API(get、index、delete、bulk、update 以及 mget)都接受一个叫做 routing 的路由参数，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。

5）Replia：副本

为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。

当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。

Elasticsearch 禁止同一个分片的主分片和副本分片在同一个节点上，所以如果是一个节点的集群是不能有副本的。

它在节点失败的情况下提供高可用性。由于这个原因，需要注意的是，副本分片永远不会分配到与主分片相同的节点上。

![x](http://121.196.182.26:6100/public/images/es_cluster.png)

详解：

1、我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点

2、新建索引和删除请求都是写操作，它们必须在主分片上成功完成才能赋值到相关的复制分片上

3、在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤：

- 客户端给 Node1 发送新建、索引或删除请求。
- 节点使用文档的 _id 确定文档属于分片0，转发请求到 Node3，分片0位于这个节点上。
- Node3 在主分片上执行请求，如果成功，它转发请求到相应的位于 Node1 和 Node2 的复制节点上。当所有的复制节点报告成功，Node3 报告成功到请求的节点，请求的节点再报告给客户端。
- 客户端接收到成功响应的时候，文档的修改已经被用于主分片和所有的复制分片，修改生效了。

ES分片复制：

复制默认的值是 sync。这将导致主分片得到复制分片的成功响应后才返回。

如果你设置 replication 为 async，请求在主分片上被执行后就会返回给客户端。它依旧会转发给复制节点，但你将不知道复制节点成功与否。

上面的这个选项不建议使用。默认的 sync 复制允许 ES 强制反馈传输。async 复制可能会因为在不等待其它分片就绪的情况下发送过多的请求而使 ES 过载。

6）全文检索

全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于 mysql 里的 like 语句。

全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如 “你们的激情是因为什么事情来的” 可能会被分词成：“你们”，“激情”，“什么事情”，“来” 等token，这样当你搜索 “你们” 或者 “激情” 都会把这句搜出来。

**核心概念**

关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns)

Elasticsearch ⇒ 索引(Index) ⇒ 类型(type) ⇒ 文档(Docments) ⇒ 字段(Fields)

1、关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）

2、一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type）

3、一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。

4、在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。

6、在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET.

**简单示例**

第一个需求是存储员工数据。 这会以“员工文档“的形式存储：一个文档代表一个员工。存储数据到 ES 的行为叫做 索引，但在索引一个文档之前，需要确定将文档存储在哪里。

一个 ES 集群可以 包含多个索引，每个索引可以包含多个类型 。 这些不同的类型存储着多个文档，每个文档又有 多个 属性 。

对于员工目录，我们可以做如下操作：

- 每个员工索引一个文档，文档包含该员工的所有信息。
- 每个文档都将是 `employee` *类型* 。
- 该类型位于索引 `leadchina` 内。
- 该索引保存在我们的 ES 中。



search返回结果不仅告知匹配了哪些文档，还包含了整个文档本身：显示搜索结果给最终用户所需的全部信息。

接下来，我们搜索姓氏为“张“的雇员。这个一般涉及到一个 查询字符串（query-string）搜索，我们通过一个URL参数来传递查询信息给搜索接口。

Query-string 搜索通过命令非常方便地进行临时性的即席搜索 ，但它有自身的局限性（）。Elasticsearch 提供一个丰富灵活的查询语言叫做“查询表达式“，它支持构建更加复杂和健壮的查询。

领域特定语言（DSL）：使用 JSON 构造了一个请求。我们可以像这样重写之前的查询所有姓为 “张” 的搜索。

ES 默认按照相关性得分排序，即每个文档跟查询的匹配程度。第一个最高得分的结果很明显：张三 的 `about` 属性清楚地写着 “高强” 。

但为什么 张三丰 也作为结果返回了呢？原因是他的 `about` 属性里提到了 “高” 。因为只有 “高” 而没有 “强” ，所以他的相关性得分低于张三的。

这个案例，阐明了 ES 如何 在 全文属性上搜索并返回相关性最强的结果。ES 中的  `相关性` 概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。

找出一个属性中的独立单词是没有问题的，但有时候想要精确匹配一系列单词或者短语。比如，刚才查询的“兄弟”，为此对 `match` 查询稍作调整，使用一个叫做 `match_phrase` 的查询，毫无悬念，返回结果仅有“张三”的文档。

许多应用都倾向于在每个搜索结果中 **高亮** 部分文本片段，以便让用户知道为何该文档符合查询条件。在 ES 中检索出高亮片段只需要增加一个新的 `highlight` 参数。当执行该查询时，返回结果与之前一样，与此同时结果中还多了一个叫做 `highlight` 的部分。这个部分包含了 `about` 属性匹配的文本片段，并以 HTML 标签 `<em></em>` 封装。

ES 有一个功能叫聚合（aggregations），允许我们基于数据生成一些精细的分析结果。聚合与 SQL 中的 `GROUP BY` 类似但更强大。举个例子，挖掘出员工中最受欢迎的兴趣爱好：

---

**ES架构简介**

一个运行中的 Elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 `cluster.name` 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。

当一个节点被选举成为 *主* 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。

作为用户，我们可以将请求发送到 *集群中的任何节点* ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。

Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 *集群健康* ， 它在 `status` 字段中展示为 `green` 、 `yellow` 或者 `red` 。

`status` 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：

- **`green`**

  所有的主分片和副本分片都正常运行。

- **`yellow`**

  所有的主分片都正常运行，但不是所有的副本分片都正常运行。

- **`red`**

  有主分片没能正常运行。

我们往 Elasticsearch 添加数据时需要用到 *索引* —— 保存相关数据的地方。 索引实际上是指向一个或者多个物理 *分片* 的 *逻辑命名空间* 。

一个 *分片* 是一个底层的 *工作单元* ，它仅保存了全部数据中的一部分。 在[`分片内部机制`](https://www.elastic.co/guide/cn/elasticsearch/guide/current/inside-a-shard.html)中，我们将详细介绍分片是如何工作的，而现在我们只需知道一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。

Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。

一个分片可以是 *主* 分片或者 *副本* 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。

一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。

在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。

让我们在包含一个空节点的集群内创建名为 `blogs` 的索引。 索引在默认情况下会被分配5个主分片， 但是为了演示目的，我们将分配3个主分片和一份副本（每个主分片拥有一个副本分片）：

集群的健康状况为 `yellow` 则表示全部 *主* 分片都正常运行（集群可以正常服务所有请求），但是 *副本* 分片没有全部处在正常状态。 实际上，所有3个副本分片都是 `unassigned` —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。

当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。

```sh
docker run -d \
           -p 9202:9200 -p 9302:9300 \
           -e ES_JAVA_POTS="-Xms128m -Xmx128m" \
           -e "discovery.type=single-node" \
           -e "cluster.name=elasticsearch" \
           --name es-node2 \
           --net exceptionless-500_default \
           elasticsearch:7.8.0
```

当第二个节点加入到集群后，3个 *副本分片* 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。

所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。

`cluster-health` 现在展示的状态为 `green` ，这表示所有6个分片（包括3个主分片和3个副本分片）都在正常运行。

我们的集群现在不仅仅是正常运行的，并且还是高可用的。

怎样为我们的正在增长中的应用程序按需扩容呢？ 当启动了第三个节点，`Node 1` 和 `Node 2` 上各有一个分片被迁移到了新的 `Node 3` 节点，现在每个节点上都拥有2个分片，而不是之前的3个。 这表示每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片的性能将会得到提升。

分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。

在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把副本数从默认的 `1` 增加到 `2` ：`blogs` 索引现在拥有9个分片：3个主分片和6个副本分片。 这意味着我们可以将集群扩容到9个节点，每个节点上一个分片。相比原来3个节点时，集群搜索性能可以提升 *3* 倍。但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。

 ES 可以应对节点故障，我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： Node 2 。

在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。

幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。

为什么我们集群状态是 yellow 而不是 green 呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应2份副本分片，而此时只存在一份副本分片。 所以集群不能为 green 的状态，不过我们不必过于担心：如果我们同样关闭了 Node 2 ，我们的程序 依然 可以保持在不丢任何数据的情况下运行，因为 Node 3 为每一个分片都保留着一份副本。

如果我们重新启动 Node 1 ，集群可以将缺失的副本分片再次进行分配。如果 Node 1 依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。

到目前为止，你应该对分片如何使得 ES 进行水平扩容以及数据保障等知识有了一定了解。 接下来我们将讲述关于分片生命周期的更多细节。





**Gateway**代表ElasticSearch索引的持久化存储方式。

　　　　　　在Gateway中，ElasticSearch默认先把索引存储在内存中，然后当内存满的时候，再持久化到Gateway里。当ES集群关闭或重启的时候，它就会从Gateway里去读取索引数据。比如LocalFileSystem和HDFS、AS3等。

　　**DistributedLucene Directory**，它是Lucene里的一些列索引文件组成的目录。它负责管理这些索引文件。包括数据的读取、写入，以及索引的添加和合并等。

　　**River**，代表是数据源。是以插件的形式存在于ElasticSearch中。　

　　**Mapping**，映射的意思，非常类似于静态语言中的数据类型。比如我们声明一个int类型的变量，那以后这个变量只能存储int类型的数据。

　　　　　　　　　　比如我们声明一个double类型的mapping字段，则只能存储double类型的数据。

　　　　　　　　　　**Mapping**不仅是告诉ElasticSearch，哪个字段是哪种类型。还能告诉ElasticSearch如何来索引数据，以及数据是否被索引到等。

　　**Search Moudle**，这个很简单

　　**Index Moudle**，这个很简单

　　**Disvcovery**，主要是负责集群的master节点发现。比如某个节点突然离开或进来的情况，进行一个分片重新分片等。这里有个发现机制。

　　　　　　　　　**发现机制**默认的实现方式是单播和多播的形式，即Zen，同时也支持点对点的实现。另外一种是以插件的形式，即EC2。

　　**Scripting**，即脚本语言。包括很多，这里不多赘述。如mvel、js、python等。　　　

　　**Transport**，代表ElasticSearch内部节点，代表跟集群的客户端交互。包括 Thrift、Memcached、Http等协议

  **RESTful Style API**，通过RESTful方式来实现API编程。

　　**3rd plugins**，代表第三方插件。

　　**Java(Netty)**，是开发框架。

　　**JMX**，是监控。

---

**Elasticsearch是如何做到快速索引的？**

Elasticsearch 的索引思路：***将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数***。

Elasticsearch 是通过 Lucene 的倒排索引技术实现比关系型数据库更快的过滤。倒排索引很多地方都有介绍，但是其比关系型数据库的 b-tree 索引快在哪里？到底为什么快呢？

这里有好几个概念。我们来看一个实际的例子，假设有如下的数据：

```txt
docid 年龄 性别
1 18 女
2 20 女
3 18 男
```

这里每一行是一个 document。每个 document 都有一个 docid。那么给这些 document 建立的倒排索引就是：

```txt
年龄
18 [1,3]
20 [2]
性别
女 [1,2]
男 [3]
```

可以看到，倒排索引是 **per field** 的，一个字段有一个自己的倒排索引。18,20 这些叫做 term，而 [1,3] 就是 posting list。Posting list 就是一个 int 的数组，存储了所有符合某个 term 的文档id。那么什么是 term dictionary 和 term index？

假设我们有很多个term，比如：

```txt
姓名：Carla,Sara,Elin,Ada,Patty,Kate,Selena
```

如果按照这样的顺序排列，找出某个特定的 term 一定很慢，因为 term 没有排序，需要全部过滤一遍才能找出特定的 term。排序之后就变成了：

```txt
Ada,Carla,Elin,Kate,Patty,Sara,Selena
```

这样我们可以用二分查找的方式，比全遍历更快地找出目标的 term。这个就是 term dictionary。有了 term dictionary 之后，可以用 $logN$ 次磁盘查找得到目标。但是磁盘的随机读操作仍然是非常昂贵的（一次 random access 大概需要 10ms 的时间）。所以尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个 term dictionary 本身又太大了，无法完整地放到内存里。于是就有了 term index。term index 有点像一本字典的大的章节表。比如：

```txt
A开头的term ……………. Xxx页
C开头的term ……………. Xxx页
E开头的term ……………. Xxx页
```

如果所有的 term 都是英文字符的话，可能这个 term index 就真的是 26 个英文字符表构成的了。但是实际的情况是，term 未必都是英文字符，term 可以是任意的 byte 数组。而且 26个 英文字符也未必是每一个字符都有均等的 term，比如 x 字符开头的 term 可能一个都没有，而 s 开头的 term 又特别多。实际的 term index 是一棵 trie 树：

![x](http://121.196.182.26:6100/public/images/es_tree.png)

例子是一个包含 "A", "to", "tea", "ted", "ten", "i", "in", 和 "inn" 的 trie 树。这棵树不会包含所有的 term，它包含的是 term 的一些前缀。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。再加上一些压缩技术（搜索 Lucene Finite State Transducers） term index 的尺寸可以只有所有 term 的尺寸的几十分之一，使得用内存缓存整个 term index 变成可能。整体上来说就是这样的效果。

![x](http://121.196.182.26:6100/public/images/es_index.png)

现在我们可以回答 **“为什么Elasticsearch/Lucene检索可以比mysql快“** 了。Mysql 只有 term dictionary 这一层，是以 b-tree 排序的方式存储在磁盘上的。检索一个 term 需要若干次的 random access 的磁盘操作。而 Lucene在 term dictionary 的基础上添加了 term index 来加速检索，term index 以树的形式缓存在内存中。从 term index 查到对应的 term dictionary 的 block 位置之后，再去磁盘上找 term，大大减少了磁盘的 random access 次数。

额外值得一提的两点是：term index 在内存中是以FST（finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary 在磁盘上是以分 block 的方式保存的，一个 block 内部利用公共前缀压缩，比如都是 Ab 开头的单词就可以把 Ab 省去。这样 term dictionary 可以比 b-tree 更节约磁盘空间。



Elasticsearch 不仅仅只是全文搜索，还包含结构化搜索、数据分析、复杂的人类语言处理、地理位置和对象间关联关系等。