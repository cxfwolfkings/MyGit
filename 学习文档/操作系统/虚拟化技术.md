# 虚拟化技术

## KVM

![x](D:\WorkingDir\Office\Resources\ag0012.jpg)

其中，KVM 全称是 基于内核的虚拟机（Kernel-based Virtual Machine），它是一个 Linux 的一个内核模块，该内核模块使得 Linux 变成了一个 Hypervisor：

- 它由 Quramnet  开发，该公司于 2008年被 Red     Hat 收购。
- 它支持 x86     (32 and 64 位), s390, Powerpc 等 CPU。
- 它从     Linux 2.6.20 起就作为一模块被包含在 Linux 内核中。
- 它需要支持虚拟化扩展的 CPU。
- 它是完全开源的。[官网](http://www.linux-kvm.org/page/Main_Page)。

本文介绍的是基于 X86 CPU 的 KVM。

### KVM架构

KVM 是基于虚拟化扩展（Intel VT 或者 AMD-V）的 X86 硬件的开源的 Linux 原生的全虚拟化解决方案。KVM 中，虚拟机被实现为常规的 Linux 进程，由标准 Linux 调度程序进行调度；虚机的每个虚拟 CPU 被实现为一个常规的 Linux 进程。这使得 KMV 能够使用 Linux 内核的已有功能。

 但是，KVM 本身不执行任何硬件模拟，需要客户空间程序通过 /dev/kvm 接口设置一个客户机虚拟服务器的地址空间，向它提供模拟的 I/O，并将它的视频显示映射回宿主的显示屏。目前这个应用程序是 QEMU。

Linux 上的用户空间、内核空间和虚机：

![x](D:\WorkingDir\Office\Resources\ag0013.jpg)

- Guest：客户机系统，包括CPU（vCPU）、内存、驱动（Console、网卡、I/O 设备驱动等），被 KVM     置于一种受限制的 CPU 模式下运行。
- KVM：运行在内核空间，提供CPU 和内存的虚级化，以及客户机的 I/O  拦截。Guest 的 I/O     被 KVM 拦截后，交给  QEMU 处理。
- QEMU：修改过的为 KVM  虚机使用的 QEMU 代码，运行在用户空间，提供硬件 I/O  虚拟化，通过 IOCTL /dev/kvm  设备和 KVM 交互。

**KVM** **是实现拦截虚机的** **I/O** **请求的原理**

现代 CPU 本身对特殊指令的截获和重定向的硬件支持，甚至新的硬件会提供额外的资源来帮助软件实现对关键硬件资源的虚拟化从而提高性能。

以 X86 平台为例，支持虚拟化技术的 CPU 带有特别优化过的指令集来控制虚拟化过程。通过这些指令集，VMM 很容易将客户机置于一种受限制的模式下运行，一旦客户机视图访问物理资源，硬件会暂停客户机的运行，将控制权交回给 VMM 处理。

VMM 还可以利用硬件的虚级化增强机制，将客户机在受限模式下对一些特定资源的访问，完全由硬件重定向到 VMM 指定的虚拟资源，整个过程不需要暂停客户机的运行和 VMM 的参与。

由于虚拟化硬件提供全新的架构，支持操作系统直接在上面运行，无需进行二进制转换，减少了相关的性能开销，极大简化了VMM的设计，使得VMM性能更加强大。从 2005 年开始，Intel 在其处理器产品线中推广 Intel Virtualization Technology 即 IntelVT 技术。

**QEMU-KVM**

其实 QEMU 原本不是 KVM 的一部分，它自己就是一个纯软件实现的虚拟化系统，所以其性能低下。但是，QEMU 代码中包含整套的虚拟机实现，包括处理器虚拟化，内存虚拟化，以及 KVM需要使用到的虚拟设备模拟（网卡、显卡、存储控制器和硬盘等）。 

为了简化代码，KVM 在 QEMU 的基础上做了修改。VM 运行期间，QEMU 会通过 KVM 模块提供的系统调用进入内核，由 KVM 负责将虚拟机置于处理的特殊模式运行。遇到虚机进行 I/O 操作，KVM 会从上次的系统调用出口处返回 QEMU，由 QEMU 来负责解析和模拟这些设备。 

从 QEMU 的角度看，也可以说是 QEMU 使用了 KVM 模块的虚拟化功能，为自己的虚机提供了硬件虚拟化加速。除此以外，虚机的配置和创建、虚机运行说依赖的虚拟设备、虚机运行时的用户环境和交互，以及一些虚机的特定技术比如动态迁移，都是 QEMU 自己实现的。 

**KVM**

KVM 内核模块在运行时按需加载进入内核空间运行。KVM 本身不执行任何设备模拟，需要 QEMU 通过 /dev/kvm 接口设置一个 GUEST OS 的地址空间，向它提供模拟的 I/O 设备，并将它的视频显示映射回宿主机的显示屏。它是KVM 虚机的核心部分，其主要功能是初始化 CPU 硬件，打开虚拟化模式，然后将虚拟客户机运行在虚拟机模式下，并对虚机的运行提供一定的支持。以在 Intel 上运行为例，KVM 模块被加载的时候，它：

1. 首先初始化内部的数据结构；
2. 做好准备后，KVM 模块检测当前的 CPU，然后打开 CPU     控制及存取 CR4 的虚拟化模式开关，并通过执行     VMXON 指令将宿主操作系统置于虚拟化模式的根模式；
3. 最后，KVM 模块创建特殊设备文件     /dev/kvm 并等待来自用户空间的指令。

接下来的虚机的创建和运行将是 QEMU 和 KVM 相互配合的过程。两者的通信接口主要是一系列针对特殊设备文件 dev/kvm 的 IOCTL 调用。其中最重要的是创建虚机。它可以理解成KVM 为了某个特定的虚机创建对应的内核数据结构，同时，KVM 返回一个文件句柄来代表所创建的虚机。

针对该句柄的调用可以对虚机做相应地管理，比如创建用户空间虚拟地址和客户机物理地址、真实物理地址之间的映射关系，再比如创建多个 vCPU。KVM 为每一个 vCPU 生成对应的文件句柄，对其相应地 IOCTL 调用，就可以对vCPU进行管理。其中最重要的就是“执行虚拟处理器”。通过它，虚机在 KVM 的支持下，被置于虚拟化模式的非根模式下，开始执行二进制指令。在非根模式下，所有敏感的二进制指令都被CPU捕捉到，CPU 在保存现场之后自动切换到根模式，由 KVM 决定如何处理。

除了 CPU 的虚拟化，内存虚拟化也由 KVM 实现。实际上，内存虚拟化往往是一个虚机实现中最复杂的部分。CPU 中的内存管理单元 MMU 是通过页表的形式将程序运行的虚拟地址转换成实际物理地址。在虚拟机模式下，MMU 的页表则必须在一次查询的时候完成两次地址转换。因为除了将客户机程序的虚拟地址转换了客户机的物理地址外，还要将客户机物理地址转化成真实物理地址。

**KVM功能列表**

KVM 所支持的功能包括：

- 支持CPU 和     memory 超分（Overcommit）
- 支持半虚拟化I/O （virtio）
- 支持热插拔 （cpu，块设备、网络设备等）
- 支持对称多处理（Symmetric     Multi-Processing，缩写为 SMP ）
- 支持实时迁移（Live     Migration）
- 支持 PCI     设备直接分配和 单根I/O 虚拟化 （SR-IOV）
- 支持 内核同页合并 （KSM ）
- 支持     NUMA （Non-Uniform Memory Access，非一致存储访问结构 ）

**KVM** **工具集合**

- libvirt：操作和管理KVM虚机的虚拟化 API，使用 C 语言编写，可以由 Python,Ruby,     Perl, PHP, Java 等语言调用。可以操作包括 KVM，vmware，XEN，Hyper-v,     LXC 等 Hypervisor。
- Virsh：基于     libvirt 的 命令行工具 （CLI）
- Virt-Manager：基于     libvirt 的 GUI 工具
- virt-v2v：虚机格式迁移工具
- virt-*     工具：包括 Virt-install （创建KVM虚机的命令行工具）， Virt-viewer     （连接到虚机屏幕的工具），Virt-clone（虚机克隆工具），virt-top     等
- sVirt：安全工具

**RedHat Linux KVM** **安装**

RedHat 有两款产品提供 KVM 虚拟化：

1. Red Hat Enterprise Linux：适用于小的环境，提供数目较少的KVM虚机。最新的版本包括 6.5 和 7.0.

2. Red Hat Enterprise Virtualization (RHEV)：提供企业规模的KVM虚拟化环境，包括更简单的管理、HA，性能优化和其它高级功能。最新的版本是 3.0.

  RedHat Linux KVM:

- KVM 由     libvirt API 和基于该 API的一组工具进行管理和控制。

- KVM 支持系统资源超分，包括内存和CPU的超分。RedHat     Linux 最多支持物理 CPU 内核总数的10倍数目的虚拟CPU，但是不支持在一个虚机上分配超过物理CPU内核总数的虚拟CPU。

- 支持 KSM     （Kenerl Same-page Merging 内核同页合并）

  RedHat Linux KVM 有如下两种安装方式：

**1****、在安装****RedHat Linux****时安装** **KVM**

  选择安装类型为 Virtualizaiton Host：

![x](D:\WorkingDir\Office\Resources\ag0014.jpg)

可以选择具体的 KVM 客户端、平台和工具：

![x](D:\WorkingDir\Office\Resources\ag0015.jpg)

**2****、在已有的****RedHat Linux****中安装** **KVM**

  这种安装方式要求该系统已经被注册，否则会报错：

[root@rh65 ~]# yum install qemu-kvm qemu-img

Loaded plugins: product-id, refresh-packagekit, security, subscription-manager

This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.

Setting up Install Process

Nothing to do

  你至少需要安装 qemu-kvm qemu-img 这两个包。

  \# yum install qemu-kvm qemu-img

  你还可以安装其它工具包：

  \# yum install virt-manager libvirt libvirt-python python-virtinst libvirt-client

**3****、****QEMU/KVM** **代码下载编译安装**

QEMU/KVM 的代码包括几个部分：

（1）KVM 内核模块是 Linux 内核的一部分。通常 Linux 比较新的发行版（2.6.20+）都包含了 KVM 内核，也可以从[这里](https://www.kernel.org/)得到。比如在我的RedHat 6.5 上：

```
[root@rh65 isoimages]# uname -r 2.6.32-431.el6.x86_64
[root@rh65 isoimages]# modprobe -l | grep kvm
kernel/arch/x86/kvm/kvm.ko
kernel/arch/x86/kvm/kvm-intel.ko
kernel/arch/x86/kvm/kvm-amd.ko
```

（2）用户空间的工具即 qemu-kvm。qemu-kvm 是 KVM 项目从 QEMU 新拉出的一个分支（[看这篇文章](http://wiki.qemu.org/KVM)）。在 QEMU 1.3 版本之前，QEMU 和 QEMU-KVM 是有区别的，但是从 2012 年底 GA 的 QEMU 1.3 版本开始，两者就完全一样了。

（3）Linux Guest OS virtio 驱动，也是较新的Linux 内核的一部分了。

（4）Windows Guest OS virtio 驱动，可以从[这里](http://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers)下载。

RedHat 6.5 上自带的 QEMU 太老，0.12.0 版本，最新版本都到了 2.* 了。

（1）. 参考 [这篇文章](http://www.yimiju.com/articles/531.html)，将 RedHat 6.5 的 ISO 文件当作本地源

```
mount -o loop soft/rhel-server-6.4-x86_64-dvd.iso /mnt/rhel6/
 
vim /etc/fstab
=> /root/isoimages/soft/RHEL6.5-20131111.0-Server-x86_64-DVD1.iso /mnt/rhel6 iso9660 ro,loop
```

[root@rh65 qemu-2.3.0]# cat /etc/yum.repos.d/local.repo
 [local]
 name=local
 baseurl=file:///mnt/rhel6/
 enabled=1
 gpgcjeck=0

 yum clean all
 yum update

  （2）. 安装依赖包

```
yum install gcc
yum install autoconf
yum install autoconf automake libtool
yum install -y glib* yum install zlib*
```

  （3）. 从 http://wiki.qemu.org/Download 下载代码，上传到我的编译环境 RedHat 6.5.

```
tar -jzvf qemu-2.3.0.tar.bz2
cd qemu-2.3.0 ./configure
make -j 4 make install
```

  （4）. 安装完成

```
[root@rh65 qemu-2.3.0]# /usr/local/bin/qemu-x86_64 -version
qemu-x86_64 version 2.3.0, Copyright (c) 2003-2008 Fabrice Bellard
```

  （5）. 为方便起见，创建一个link

ln -s /usr/bin/qemu-system-x86_64 /usr/bin/qemu-kvm

  **安装** **libvirt**

可以从[ libvirt 官网](ftp://libvirt.org/libvirt/)下载安装包。最新的版本是 0.10.2. 

**创建** **KVM** **虚机的几种方式**

**使用** **virt-install** **命令**

virt-install \

--name=guest1-rhel5-64 \

--file=/var/lib/libvirt/images/guest1-rhel5-64.dsk \

--file-size=8 \

--nonsparse --graphics spice \

--vcpus=2 --ram=2048 \

--location=http://example1.com/installation_tree/RHEL5.6-Serverx86_64/os \

--network bridge=br0 \

--os-type=linux \

--os-variant=rhel5.4

**使用** **virt-manager** **工具**

![x](D:\WorkingDir\Office\Resources\ag0016.jpg)

![x](D:\WorkingDir\Office\Resources\ag0017.jpg)

使用 VMM GUI 创建的虚机的xml 定义文件在 /etc/libvirt/qemu/ 目录中。

**使用** **qemu-img** **和** **qemu-kvm** **命令行方式安装**

（1）创建一个空的qcow2格式的镜像文件

  qemu-img create -f qcow2 windows-master.qcow2 10G

（2）启动一个虚机，将系统安装盘挂到 cdrom，安装操作系统

  qemu-kvm -hda windows-master.qcow2 -m 512 -boot d -cdrom /home/user/isos/en_winxp_pro_with_sp2.iso

（3）现在你就拥有了一个带操作系统的镜像文件。你可以以它为模板创建新的镜像文件。使用模板的好处是，它会被设置为只读所以可以免于破坏。 

  qemu-img create -b windows-master.qcow2 -f qcow2  windows-clone.qcow2

（4）你可以在新的镜像文件上启动虚机了

  qemu-kvm -hda windows-clone.qcow2 -m 400

**通过** **OpenStack Nova** **使用** **libvirt API** **通过编程方式来创建虚机** **（后面会介绍）**

 

[**CPU****和内存虚拟化**](https://www.cnblogs.com/sammyliu/p/4543597.html)

**1.** **为什么需要** **CPU** **虚拟化**

X86操作系统是设计在直接运行在裸硬件设备上的，因此它们自动认为它们完全占有计算机硬件。x86架构提供四个特权级别给操作系统和应用程序来访问硬件。Ring是指CPU的运行级别，Ring 0是最高级别，Ring1次之，Ring2更次之…… 就 Linux+x86 来说， 

- 操作系统（内核）需要直接访问硬件和内存，因此它的代码需要运行在最高运行级别 Ring0上，这样它可以使用特权指令，控制中断、修改页表、访问设备等等。 
- 应用程序的代码运行在最低运行级别上ring3上，不能做受控操作。如果要做，比如要访问磁盘，写文件，那就要通过执行系统调用（函数），执行系统调用的时候，CPU的运行级别会发生从ring3到ring0的切换，并跳转到系统调用对应的内核代码位置执行，这样内核就为你完成了设备访问，完成之后再从ring0返回ring3。这个过程也称作用户态和内核态的切换。

![x](D:\WorkingDir\Office\Resources\ag0018.jpg)

那么，虚拟化在这里就遇到了一个难题，因为宿主操作系统是工作在 ring0 的，客户操作系统就不能也在 ring0 了，但是它不知道这一点，以前执行什么指令，现在还是执行什么指令，但是没有执行权限是会出错的。所以这时候虚拟机管理程序（VMM）需要避免这件事情发生。 虚机怎么通过 VMM 实现 Guest CPU 对硬件的访问，根据其原理不同有三种实现技术：

\1. 全虚拟化

\2. 半虚拟化

\3. 硬件辅助的虚拟化 

**基于二进制翻译的全虚拟化（****Full Virtualization with Binary Translation****）**

![x](D:\WorkingDir\Office\Resources\ag0019.jpg)

客户操作系统运行在 Ring 1，它在执行特权指令时，会触发异常（CPU的机制，没权限的指令会触发异常），然后 VMM 捕获这个异常，在异常里面做翻译，模拟，最后返回到客户操作系统内，客户操作系统认为自己的特权指令工作正常，继续运行。但是这个性能损耗，就非常的大，简单的一条指令，执行完了事，现在却要通过复杂的异常处理过程。

异常 “捕获（trap）-翻译（handle）-模拟（emulate）” 过程：

![x](D:\WorkingDir\Office\Resources\ag0020.jpg)

**超虚拟化（或者半虚拟化****/****操作系统辅助虚拟化** **Paravirtualization****）** 

半虚拟化的思想就是，修改操作系统内核，替换掉不能虚拟化的指令，通过超级调用（hypercall）直接和底层的虚拟化层hypervisor来通讯，hypervisor 同时也提供了超级调用接口来满足其他关键内核操作，比如内存管理、中断和时间保持。

  这种做法省去了全虚拟化中的捕获和模拟，大大提高了效率。所以像XEN这种半虚拟化技术，客户机操作系统都是有一个专门的定制内核版本，和x86、mips、arm这些内核版本等价。这样以来，就不会有捕获异常、翻译、模拟的过程了，性能损耗非常低。这就是XEN这种半虚拟化架构的优势。这也是为什么XEN只支持虚拟化Linux，无法虚拟化windows原因，微软不改代码啊。

![x](D:\WorkingDir\Office\Resources\ag0021.jpg)

**硬件辅助的全虚拟化** 

  2005年后，CPU厂商Intel 和 AMD 开始支持虚拟化了。 Intel 引入了 Intel-VT （Virtualization Technology）技术。 这种 CPU，有 VMX root operation 和 VMX non-root operation两种模式，两种模式都支持Ring 0 ~ Ring 3 共 4 个运行级别。这样，VMM 可以运行在 VMX root operation模式下，客户 OS 运行在VMX non-root operation模式下。

![x](D:\WorkingDir\Office\Resources\ag0022.jpg)

而且两种操作模式可以互相转换。运行在 VMX root operation 模式下的 VMM 通过显式调用 VMLAUNCH 或 VMRESUME 指令切换到 VMX non-root operation 模式，硬件自动加载 Guest OS 的上下文，于是 Guest OS 获得运行，这种转换称为 VM entry。Guest OS 运行过程中遇到需要 VMM 处理的事件，例如外部中断或缺页异常，或者主动调用 VMCALL 指令调用 VMM 的服务的时候（与系统调用类似），硬件自动挂起 Guest OS，切换到 VMX root operation 模式，恢复 VMM 的运行，这种转换称为 VM exit。VMX root operation 模式下软件的行为与在没有 VT-x 技术的处理器上的行为基本一致；而VMX non-root operation 模式则有很大不同，最主要的区别是此时运行某些指令或遇到某些事件时，发生 VM exit。

也就说，硬件这层就做了些区分，这样全虚拟化下，那些靠“捕获异常-翻译-模拟”的实现就不需要了。而且CPU厂商，支持虚拟化的力度越来越大，靠硬件辅助的全虚拟化技术的性能逐渐逼近半虚拟化，再加上全虚拟化不需要修改客户操作系统这一优势，全虚拟化技术应该是未来的发展趋势。

|                                     | **利用二进制翻译的全虚拟化**       | **硬件辅助虚拟化**                                           | **操作系统协助****/****半虚拟化**                            |
| ----------------------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **实现技术**                        | BT和直接执行                       | 遇到特权指令转到root模式执行                                 | Hypercall                                                    |
| **客户操作系统修改****/****兼容性** | 无需修改客户操作系统，最佳兼容性   | 无需修改客户操作系统，最佳兼容性                             | 客户操作系统需要修改来支持hypercall，因此它不能运行在物理硬件本身或其他的hypervisor上，兼容性差，不支持Windows |
| **性能**                            | 差                                 | 全虚拟化下，CPU需要在两种模式之间切换，带来性能开销；但是，其性能在逐渐逼近半虚拟化。 | 好。半虚拟化下CPU性能开销几乎为0，虚机的性能接近于物理机。   |
| **应用厂商**                        | VMware Workstation/QEMU/Virtual PC | VMware ESXi/Microsoft Hyper-V/Xen 3.0/KVM                    | Xen                                                          |

**2****. KVM CPU** **虚拟化**

**Xen**

 

**OpenVZ**

 

 

**VPS****的三种虚拟技术****OpenVZ****、****Xen****、****KVM****优缺点比较**

**点评**：本文就VPS采用的几种常见技术OpenVZ、Xen、KVM做简单介绍和对比，以备你选购自己合适的VPS

很多人看到同样配置的VPS价格相差很大，甚是不理解，其实VPS使用的虚拟技术种类有很多，如OpenVZ、Xen、KVM、Xen和HVM与PV。在+XEN中pv是半虚拟化，hvm是全虚拟化，pv只能用于linux内核的系统，效率更高，hvm可以虚拟所有常见操作系统(可以使用windows)，+理论效率比pv略低，另外，hvm需要cpu虚拟化指令支持，pv无此要求。KVM是新兴的虚拟化项目，出售KVM的VPS商家不多，但据说KVM虚拟技+术是比较强悍的。而OpenVZ是一个类似于Linux-VServer的操作系级全虚拟化解决方案，目前基于Xen和OpenVZ的VPS服务商比较+多。本文就VPS采用的几种常见技术OpenVZ、Xen、KVM做简单介绍和对比，以备你选购自己合适的VPS。 

**OpenVZ** 

OpenVZ是操作系统级别的虚拟化技术，是底层操作系统上的一层应用，这意味着易于理解和低权重开销，一般来说也意味着更优的性能。但是OpenVZ配置起来比较灵活，给黑心服务商改低限制的机会。 

优势：现在外面卖的这种类型，许可的内存都超大，CPU也强劲，而且卖家很多，可比性也很强。性价比超高。 

劣势：超卖，没有不超卖的，导致各种石头盘，钻石盘。连带的cpu也被过度分割导致性能升值不如其标明的1/10。再有就是内存，基本上OpenVZ技术没有独占的，都是共享，别人多了你就少了，而且这种技术最大的劣势就是内存下来后直接就是当机。还有开设vpn各种不方便。 

**Xen** 

Xen是半虚拟化技术，它并不是一个真正的虚拟机，而是相当于自己运行了一个内核的实例，可以自由的加载内核模块，虚拟的内存和IO，稳定而且可预测。分Xen+pv+和Xen+hvm，区别就是pv只支持linux，而hvm支持win系统。 

优势：内存独占，虽然小但是保证分配的到。部分虚拟技术决定了就算是超卖也不会超太离谱。所以一般的母鸡就算超了，也很少导致石头盘。当然小鸡有pt就除外了。另外就是即使内存再低也不会当掉，只是会无响应一段时间。 

劣势：内存小，硬盘小，带宽小（以上3点均和同价位的openvz对比）。因为没法超卖了，为了赚钱只能把这3个基本配置降下来了。还有就是供应商残次不齐，而且大部分在说xen的时候都木有明说是pv还是hvm，其实pv的性能是优于hvm的。 

**KVM** 

优势：和xen类似，比xen更好的一点是，kvm是完全虚拟的，所以不分pv和hvm的区别，所有的kvm类+型的虚拟技术都可以装各种linux的发行版和各种win的发行版，不管供应商在主页有没有写明是否支持win，只要你配置够win运行，那就肯定可以装+的上去，只是方法的问题而已。 

劣势：恰恰因为kvm可以装任意类型的操作系统，导致了折腾帝甚至在128m的机器上装了win2003（上去+后直接所有的cpu和内存都用于硬盘IO了，不明白这些人想干神马）。其结果就是所有的kvm邻居都得看你周围邻居的脸色。这么说把，一个node下只要+有5、6台这种折腾帝，对不起，你的硬盘基本就是石头盘了。 

个人比较倾向于Xen，因为它是对供应商和和客户来说都是可以接受的折衷方案。openvz超卖太厉害，kvm等看RP，这2种都不适合那些不想折腾只想做站的站长。

【检测小妙招】

对于新手来说，刚开始使用VPS的时候只要这家商家不跑路，机器稳定流畅就已经很感激流涕了。随着我们学习只是的深入，以及开始需要检测自己购买的VPS是否如服务商列举出来的参数和配置以及环境。在我开始使用VPS的时候，给一个朋友帮我看，他告诉我我买的不是真的XEN VPS。

如果大家对自己购买和使用的VPS需要检测是否为真的[Xen](http://www.laozuo.org/tag/xen)，我们可以用如下方法进行测试。比较专业的就是用virt-what脚本进行检测。方法如下：

wget http://people.redhat.com/~rjones/virt-what/files/virt-what-1.12.tar.gz
 tar zxvf virt-what-1.12.tar.gz
 cd virt-what-1.12/
 ./configure
 make && make install

virt-what

**如何检测****OpenVZ VPS****和****Xen VPS****是否超售**

百分百销售VPS的主机商都有“超售”的现象，不管你是OpenVZ还是Xen环境，没有“超不超售”的问题，只是是否严重而已，如果严重的话那就是变成了"OVERLOAD"了，也就是服务器超载。所以，大家购买的时候不要心里报着有地方购买到没有超售的便宜的VPS。那是不可能的。

没有超售的VPS一般一个月的价格都可能是你便宜的VPS的半年或者一年的价格。

**查看****OpenVZ VPS****的超售方法：**

进入linux的共享内存目录:
 cd /dev/shm
 创建100MB的文件:
 dd if=/dev/zero of=./memtest bs=1M count=100
 查看内存:
 free -m
 可以看到内存使用增长了100MB。
 只要不超过提供商的内存配额，
 不断调大创建文件命令中的count值，
 可以看到内存的增长，
 假如在承诺内存内机子就挂彩就是超售非常严重了。
 结束的时候输入：
 rm ./memtest
 其他:
 查看CPU信息:
 cat /proc/cpuinfo
 磁盘IO测试命令
 dd if=/dev/zero of=test bs=64k count=512 oflag=dsync
 端口速度测试
 wget http://cachefly.cachefly.net/100mb.test
 1）查看进程内存占用
 top
 2）查看CPU信息
 cat /proc/cpuinfo
 3）磁盘IO测试命令
 dd if=/dev/zero of=test bs=64k count=4k oflag=dsync
 4）端口速度测试
 wget http://cachefly.cachefly.net/100mb.test
 5）查看Inode
 df -i
 6）内存超售检查
 进入linux的共享内存目录:
 cd /dev/shm
 创建100MB的文件:
 dd if=/dev/zero of=./memtest bs=1M count=100
 查看内存:
 free -m
 结束的时候输入：
 rm ./memtest

**查看****Xen VPS****的超售方法：**

以前购买了一个512MB内存的Xen VPS
 价格也便宜。所以，我登录并检查了多少内存。
 运行
 \# free
       total    used    free   shared  buffers   cached
 Mem:    543776   535360    8416     0   66516   130504
 -/+ buffers/cache:   338340   205436
 Swap:   1048568    136  1048432
 330MB的内存已经用于我新的Xen的VPS。”让我们来看看哪些进程正在运行
 \# ps aux
 ...
 root    204 0.0 0.1 17028  780 ?    S  Oct27  0:00 upstart-udev-bridge --daemon
 102    356 0.0 0.1 23548 1080 ?    Ss  Oct27  0:00 dbus-daemon --system --fork
 root    431 0.0 0.1 21068  788 ?    Ss  Oct27  0:00 cron
 root   3110 0.0 0.5 253832 2992 ?    Sl  Oct28  0:00 /usr/sbin/console-kit-daemon --no-daemon
 root   11037 0.0 0.1 49256 1012 ?    Ss  Oct28  0:00 /usr/sbin/sshd
 root   15427 0.0 0.1 12520  772 ?    S  Oct28  0:00 /usr/sbin/syslogd --no-forward
 root   31231 0.0 0.0 16748  436 ?    Ss Nov12  0:00 udevd --daemon
 root   5716 0.0 0.6 79100 3772 ?    Ss  01:02  0:00 sshd: root@pts/0
 root   5731 0.0 0.3 19400 2148 pts/0  Ss  01:02  0:00 -bash
 root   5782 0.0 0.1  6072  724 ?    Ss  01:08  0:00 /sbin/getty -8 38400 hvc0
 root   5783 0.0 0.2 15248 1172 pts/0  R+  01:08  0:00 ps --sort=start_time uax
 没有什么内存在VPS上运行。 SSH服务器，syslogd的，cron
 那么，我没有使用的内存为什么330MB消失？
 让我们进一步的挖掘之前，我们得出这个认证。
 你可以肯定出答案：该Xen的VPS的内存存在于超售行为。这东西是好多年来很多人都知道，它使用一个称为“ ballooning”的技术。
 基本上是一个特殊的Linux内核驱动程序安装在您的系统 – “balloon driver”。
 当dom0的（在Xen服务器/管理程序）需要更多的内存，并希望要求从（domU）客户的VPS，它会要求客人VPS的“balloon driver”充当本身
 \- 通过询问其Linux内核的一些内存。内核内存分配将被要求到该VPS系统可用的内存，并且不能调出到交换。
 主机一旦膨胀的消耗内存，然后将其传递给dom0/hypervisor用于其他地方（例如建立一个新的VPS）。
 因此，你的VPS的“总内存”将保持不变，但将在“内存使用”大增加，一大块已经被内核balloon driver驱动程序使用