# 先导内网架构笔记

1. 安装部署
   - [mysql部署](#mysql部署)
2. 运维监控
   - Linux命令
     - [网络设置](#网络设置)
     - [软件安装](#软件安装)
     - [查看服务器信息](#查看服务器信息)
3. [CentOS7安装Gitlab](#CentOS7安装Gitlab)
4. [kubeamd部署k8s集群](#kubeamd部署k8s集群)
5. [k8s部署consul集群](#k8s部署consul集群)
6. [dotnet服务部署](#dotnet服务部署)
7. [k8s常用命令](#k8s常用命令)
8. [Helm](#Helm)
9. [CICD](#CICD)



## 安装部署



### mysql部署

**单体搭建：**

创建数据目录

```sh
mkdir -p /data/mysql/1/data /data/mysql/1/conf /data/mysql/1/logs
mkdir -p /root/ftp/mysql
vim /root/ftp/mysql/docker-compose.yml
```

docker-compose.yml

```yaml
version: '3.7'

services:
  mysql:
    image: mysql
    container_name: mysql
    command:
      --default-authentication-plugin=mysql_native_password
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_general_ci
      --lower_case_table_names=1
    restart: unless-stopped # docker的重启策略：在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器
    environment:
      MYSQL_ROOT_PASSWORD: 123 # root用户的密码
      MYSQL_USER: copy # 创建新用户
      MYSQL_PASSWORD: 123 # 新用户的密码
    ports:
      - 3306:3306
    volumes:
      - /data/mysql/1/data:/var/lib/mysql
      - /data/mysql/1/conf:/etc/mysql/conf.d
      - /data/mysql/1/logs:/logs
    networks:
      default:
        ipv4_address: 172.18.1.201

networks:
  default:
    external:
      name: lead_pm1
```

生成容器

```sh
cd /root/ftp/mysql && docker-compose up -d && cd /root/ftp
cd /root/ftp/mysql && docker-compose ps && cd /root/ftp
```

导入准备：

```sql
SET GLOBAL log_bin_trust_function_creators = 1
```



## 运维监控



### Linux命令



#### 网络设置

```sh
# 图形化设置
nmtui
# 重启网络
systemctl restart network
```

查看当前服务器公网IP

```sh
curl ifconfig.me
curl cip.cc
curl icanhazip.com
curl ident.me
curl ipecho.net/plain
curl whatismyip.akamai.com
curl tnx.nl/ip
curl myip.dnsomatic.com
curl ip.appspot.com
curl -s checkip.dyndns.org | sed 's/.*IP Address: \([0-9\.]*\).*/\1/g'
```



#### 软件安装

通用软件安装方法：

1、下载RPM安装文件

- RPM Find：[http://www.rpmfind.net/](http://www.rpmfind.net/)

- `wget <rpm_url>`

- wget命令不存在时：`yum install wget`

2、安装RPM文件

- `yum install <xxx.rpm>`

**源码安装**

下载源码包，解压tar.gz包，`tar –xvzf 软件包名`。

```sh
# 将软件包名.tar.gz解压到指定的目录下（注意：-C为大写）
tar -zxvf xxx.tar.gz -C /home/colin
```

 进入解压后的文件目录下

```sh
# 为编译做好准备
./configure
# 表示将要安装到/opt目录
./configure --prefix=/opt
# 软件编译
make
# 软件安装
make install
# 删除安装时产生的临时文件
make clean
# 软件卸载
make uninstall
```

**RPM安装**

软件包：

1. bin文件.bin
2. rpm包
3. 源码压缩包

**rpm**管理软件

| 命令                   | 说明                                                         |
| ---------------------- | ------------------------------------------------------------ |
| rpm -i example.rpm     | 安装 example.rpm 包；                                        |
| rpm -iv example.rpm    | 安装 example.rpm 包并在安装过程中显示正在安装的文件信息；    |
| rpm -ivh example.rpm   | 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度； |
| rpm -qa \| grep gitlab | 查看安装完成的软件                                           |
| rpm -e --nodeps        | 要卸载的软件包                                               |

RPM默认安装路径：

- /etc：一些设置文件放置的目录如/etc/crontab

- /usr/bin：一些可执行文件

- /usr/lib：一些程序使用的动态函数库

- /usr/share/doc：一些基本的软件使用手册与帮助文档

- /usr/share/man：一些man page文件

**yum安装**

 `Yum` 仓库则是为进一步简化 `RPM` 管理软件难度而设计的，`Yum` 能够根据用户的要求分析出所需软件包及其相关依赖关系，自动从服务器下载软件包并安装到系统。

 用户能够根据需求来指定 `Yum` 仓库与是否校验软件包，而这些只需几条关键词即可完成，现在来学习下配置的方法：所有 `Yum` 仓库的配置文件均需以 `.repo` 结尾并存放在 `/etc/yum.repos.d/` 目录中。

```ini
[rhel-media]：yum源的名称，可自定义。
baseurl=file:///media/cdrom：提供方式包括FTP(ftp://..)、HTTP(http://..)、本地(file:///..)
enabled=1：设置此源是否可用，1为可用，0为禁用。
gpgcheck=1：设置此源是否校验文件，1为校验，0为不校验。
gpgkey=file:///media/cdrom/RPM-GPG-KEY-redhat-release：若为校验请指定公钥文件地址。
```

`Yum` 仓库中的 `RPM` 软件包可以是由红帽官方发布的，也可以是第三方组织发布的，当然用户也可以编写的~

| 标识        | 简写 | 前景色    | 后景色 | 说明                                                         |
| ----------- | ---- | --------- | ------ | ------------------------------------------------------------ |
| Debug       | dbug | Gray      | Black  | 在开发过程中用于交互式调查的日志。这些日志应主要包含对调试有用的信息，不具有长期价值。 |
| Information | info | DarkGreen | Black  | 跟踪应用程序的一般流程的日志。这些日志应具有长期价值。       |
| Warning     | warn | Yellow    | Black  | 突出显示应用程序流中异常或意外事件的日志，但是否则不会导致应用程序执行停止。 |
| Error       | fail | Red       | Black  | 当当前执行流程由于失败而停止时，会突出显示的日志。这些应该指示当前活动中的故障，而不是应用程序范围的故障。 |
| Critical    | cril | White     | Red    | 描述不可恢复的应用程序或系统崩溃或灾难性的日志失败需要立即关注。 |
| None        |      |           |        | 不用于写日志消息。 指定记录类别不应写任何消息。              |

| 命令                        | 作用                         |
| --------------------------- | ---------------------------- |
| `yum repolist all`          | 列出所有仓库                 |
| `yum list all`              | 列出仓库中所有软件包         |
| `yum info 软件包名称`       | 查看软件包信息               |
| `yum install 软件包名称`    | 安装软件包                   |
| `yum reinstall 软件包名称`  | 重新安装软件包               |
| `yum update 软件包名称`     | 升级软件包                   |
| `yum remove 软件包`         | 移除软件包                   |
| `yum clean alla`            | 清除所有仓库缓存             |
| `yum check-update`          | 检查可更新的软件包           |
| `yum grouplist`             | 查看系统中已经安装的软件包组 |
| `yum groupinstall 软件包组` | 安装指定的软件包组           |
| `yum groupremove 软件包组`  | 移除指定的软件包组           |
| `yum groupinfo 软件包组`    | 查询指定的软件包组信息       |

**安装虚拟机增加包**

VMware Tools是VMware虚拟机中自带的增强工具包，用于增强虚拟机显卡与硬盘性能、同步虚拟机与主机的时钟时间、最主要的是可以支持虚拟机与主机之间的文件拖拽传输。

第1步：在虚拟软件中选择“安装/重新安装VMware Tools(T)”

第2步：安装VMwareTools功能增加包（请用root用户登陆系统）

```sh
# 创建/media/cdrom目录：
mkdir -p /media/cdrom
# 将光驱设备挂载到该目录上：
mount /dev/cdrom /media/cdrom
# 进入到该挂载目录：
cd /media/cdrom
# 将功能增强包复制到/home目录中：
cp VMwareTools-10.3.2-9925305.tar.gz /home
# 进入到/home目录中：
cd /home
# 解压功能增强包：
tar xzvf VMwareTools-10.3.2-9925305.tar.gz
…………………………………………………………………………………………………………………………………………………………………………
vmware-tools-distrib/
vmware-tools-distrib/FILES
...
……………………………………………………………………此处省略解压过程细节…………………………………………………

# 进入解压文件夹中：
cd vmware-tools-distrib/
# 运行安装脚本（加上参数-d，代表默认安装，这里需要手动安装）：
./vmware-install.pl
…………………………………………………………………………………………………………………………………………………………………………
The installer has detected an existing installation of open-vm-tools on this system and will not attempt to remove and replace these user-space applications. It is recommended to use the open-vm-tools packages provided by the operating system. If you do not want to use the existing installation of open-vm-tools and attempt to install VMware Tools, you must uninstall the open-vm-tools packages and re-run this installer.
The installer will next check if there are any missing kernel drivers. Type yes if you want to do this, otherwise type no [yes]
……………………………………………………………………省略部分安装过程……………………………………………………………

# 当您看到这个字样后，重启后即可正常使用VmwareTools啦
…………………………………………………………………………………………………………………………………………………………………………
Creating a new initrd boot image for the kernel.
Starting Virtual Printing daemon: done
Starting vmware-tools (via systemctl): [ OK ]
The configuration of VMware Tools 9.9.0 build-2304977 for Linux for this running kernel completed successfully.
Enjoy,
--the VMware team
…………………………………………………………………………………………………………………………………………………………………………
```

可能会遇到的问题：

```sh
bash: ./vmware-install.pl: /user/bin/perl: 坏的解释器:没有那个文件或目录

# 解决方法
yum install perl gcc kernel-devel
yum upgrade kernel kernel-devel

# 如果出现
…………………………………………………………………………………………………………………………………………………………………………
‍Searching for a valid kernel header path…
The path "" is not valid.
…………………………………………………………………………………………………………………………………………………………………………
# 这是因为 kernel-devel 版本和相应的 kernel 版本不一致，可以用 uname-r 看一下内核版本，再用 rpm -q kernel-devel 看一下 kernel-devel 的版本，有可能会出现 kernel-devel 未找到的错误，这里需要自己安装一下，可以执行 sudo yum install kernel-devel，这个时候会安装最新的 kernel-devel 版本，重启一下，如果再出现问题，那么可以执行 sudo yum upgrade kernel kernel-devel，把内核和 kernel-devel 更新到同一个版本，这样应该就不会有问题了。而 GCC 和 PERL 的问题提示比较简单。
# 建议在安装之前还是执行一下安装 GCC 和 PERL，执行发下命令：yum install perl gcc kernel-devel
```

第3步：重新启动系统后生效：`reboot`

```sh
# 此时在linux中进入 /mnt/hgfs 文件夹，但发现共享的文件没有显示，继续。
vmware-hgfsclient
-bash: vmware-hgfsclient: 未找到命令
# share是共享文件夹名称
mount -t vmhgfs .host:/share /mnt/hgfs
temp 的密码：
Error: cannot mount filesystem: No such device（如提示该错误）
# 安装 yum install open-vm-tools
yum install open-vm-tools
# 完成后，再执行以下命令，就有共享文件夹啦
vmhgfs-fuse .host:/ /mnt/hgfs
# 查看共享文件夹
vmware-hgfsclient
```



#### 查看服务器信息

```sh
# 查看分区和磁盘
lsblk
# 查看空间使用情况
df -h
# 分区工具查看分区信息
fdisk -l
# 查看分区
cfdisk /dev/sda
# 查看硬盘label（别名）
blkid
# 统计当前目录各文件夹大小
du -sh ./*
# 查看内存大小
free -h
# 查看cpu核心数
cat /proc/cpuinfo| grep "cpu cores"| uniq

# 查看服务器所有被占用端口
netstat -ant
# 验证某个端口号是否被占用
netstat -tunlp | grep 端口号
# 查看所有监听端口号
netstat -lntp
```



## CentOS7安装Gitlab

```sh
sudo yum install -y curl policycoreutils-python openssh-server openssh-clients perl
sudo systemctl enable sshd
sudo systemctl start sshd
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo systemctl reload firewalld

sudo yum install postfix
sudo systemctl enable postfix
sudo systemctl start postfix

curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash

sudo EXTERNAL_URL="http://10.120.68.12" yum install -y gitlab-ce

# 修改gitlab配置文件，指定服务器ip和自定义端口
vim /etc/gitlab/gitlab.rb
# 重置配置
gitlab-ctl reconfigure
# 重启gitlab服务
gitlab-ctl restart

# 使用SSH
ssh-keygen -t ed25519 -C "projectmanage_dq@leadchina.cn"
# 添加或更改密钥对的密码
ssh-keygen -p -o -f <keyname>
# 复制密码
xclip -sel clip < ~/.ssh/id_ed25519.pub
# 查看key
cat /root/.ssh/id_ed25519.pub
# 查看是否配置成功
ssh -T git@10.120.68.12
```

参考：

- https://blog.csdn.net/li_wen_jin/article/details/107096737?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242



## kubeamd部署k8s集群

更改yum源为阿里云的yum源

```sh
cd /etc/yum.repos.d/
# 备份原文件
mv CentOS-Base.repo CentOS-Base.repo.bak

# Centos7
wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
# 网易源：wget -O CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo

# 将服务器上的软件包信息缓存到本地,以提高搜索安装软件的速度
yum makecache

# 如果你在执行上面这边命令时，报错：Error: Cannot retrieve metalink for repository: epel. Please verify its path and try again
# 建议用如下方法解决：检查/etc/yum.repos.d/下是否有epel.repo文件，如果有，重命名为epel.repo_bak，千万不能以.repo格式备份，然后在执行一次上面的命令即可！
```

添加清华源：

```sh
vim /etc/yum.repos.d/CentOS-Base.repo
# ---------------------------------------------------------------------------------------------
# CentOS-Base.repo
#
# The mirror system uses the connecting IP address of the client and the
# update status of each mirror to pick mirrors that are updated to and
# geographically close to the client.  You should use this for CentOS updates
# unless you are manually picking other mirrors.
#
# If the mirrorlist= does not work for you, as a fall back you can try the
# remarked out baseurl= line instead.
#
#

[base]
name=CentOS-$releasever - Base
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7

#released updates
[updates]
name=CentOS-$releasever - Updates
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7

#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7

#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus
gpgcheck=0
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-7
# ---------------------------------------------------------------------------------------------
```

### 一. 初始化操作

1、首先关闭服务器的防火墙

```bash
systemctl stop firewalld     # 永久关闭
systemctl disable firewalld  # 永久关闭

# 永久关闭selinux
sed -i 's/enforcing/disabled/' /etc/selinux/config
# 关闭swap分区
swapoff -a                            # 临时
sed  -ri 's/.*swap.*/#&/' /etc/fstab  # 永久关闭
```

2、给主机添加主机名称方便管理

```bash
Hostnamectl  ecs-K8s-master
Hostnamectl  ecs-K8s-node1
Hostnamectl  ecs-K8s-node2

# 在master服务器添加hosts，其他不用添加
# /etc/hosts配置文件中加入你的ip及修改的名字
vim /etc/hosts
# --------------------------------------
192.168.0.1 ecs-K8s-master
192.168.0.2 ecs-K8s-node1
192.168.0.3 ecs-K8s-node2
# --------------------------------------
# 重启
# /etc/init.d/network restart
systemctl restart network
```

3、将桥接的IPV4流量传递到iptanles的链

```bash
# /etc/sysctl.d/k8s.conf 服务器的配置文件加入下面两行。
vim /etc/sysctl.d/k8s.conf
# ---------------------------------------
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1
# --------------------------------------

# 执行如下立即生效
sysctl --system
```

4、服务器同步时间

```bash
yum -y install ntpdate
ntpdate time.windows.com
```

### 二、所有节点的准备工作

1、所有节点安装docker

```bash
wget -i http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 拷贝
cp docker-ce.repo /etc/yum.repos.d/

# 安装所需软件包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 查看各版本docker
yum list docker-ce --showduplicates | sort -r
# 安装dockers
yum install docker-ce-18.06.1.ce-3.el7 -y
# 启动dockers
systemctl start docker
```

2、Docker中的仓库配置（所有节点）

```bash
cat >/etc/docker/daemon.json << EOF
{
  "registry-mirrors":["https://b9pmyelo.mirror.aliyuncs.com"]
}
EOF

# 配置完成需要重启docker
systemctl restart docker
```

3、添加阿里云yum软件源（所有节点执行）

```bash
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

4、部署kubeadm,kubelet和kubectl（所有节点执行）

```bash
# 由于版本更新频繁，这里指定版本号部署
yum clean all
rpm --rebuilddb
yum -y makecache
yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0

# 检查是否安装
rpm -qa | grep kube

# 启动kubelet，并设置开机启动
systemctl enable kubelet && systemctl start kubelet
```

5、初始化master（在master上面执行）

```bash
# apiserver-advertise-address指定master的ip地址
# pod-network-cidr指定Pod网络的范围，这里使用flannel网络方案
kubeadm init --kubernetes-version=1.18.0 --apiserver-advertise-address=192.168.0.1 --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16
```

6、加入kubernetes node

```bash
# 更改设置，令容器运行时和kubelet使用systemd作为cgroup驱动，以此使系统更为稳定
vim /etc/docker/daemon.json
# -------------------------------------------------------------------------
{
  "registry-mirrors":["https://b9pmyelo.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
# -------------------------------------------------------------------------
systemctl daemon-reload && systemctl restart docker
# 设置完成后通过docker info命令可以看到Cgroup Driver为systemd
docker info | grep Cgroup

# 加入master集群，根据提示执行即可
kubeadm join 10.120.68.11:6443 --token vqn1ry.hbg7bkj5tzpgjoq9 --discovery-token-ca-cert-hash sha256:868eaa647e0515b67221e4303849ddfe08a87182a98cd5c20291dd46b91e6800

# 如果token失效，需要master重新生成
kubeadm token list
kubeadm token create
# 获取sha256
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

# 在master节点执行，查看是否有节点加入
kubectl get nodes
```

7、部署CNI网络插件（在master执行）

```bash
Kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# 查看是否执行完成已经运行
kubectl get pods -n kube-system
```

![x](C:\Resources\k8s001.png)

上面的状态全部为1/1时，使用`kubectl get nodes`，查看是否是已经就绪

![x](C:\Resources\k8s002.png)

集群已经搭建完成。

8、测试kubernetes集群

在kubernetes集群中创建一个pod，验证是否正常运行。

```bash
kubectl create deployment nginx --image=nginx  # 拉取一个nginx
kubectl get pod  # 查看是否拉取完成。Running已经拉取完成

# 当状态是running时执行对外进行端口暴露。
kubectl expose deployment nginx --port=80 --type=NodePort

kubectl get pod,svc  # 查看对外的端口
```

![x](C:\Resources\k8s003.png)

测试：master的ip加上31667可以访问，node节点的ip加上31667也可以访问。

k8s集群搭建完成。



参考：

- https://blog.csdn.net/ahilll/article/details/81979947



## k8s部署consul集群

Consul是由HashiCorp基于Go语言开发的支持多数据中心分布式高可用的服务发布和注册服务软件，采用Raft算法保证服务的一致性，且支持健康检查。但是在kubernetes里，当节点发生故障或者资源不足时，会根据策略杀掉节点的一些pod转而生成新的pod，而新生成的pod的ip地址和名称（hash值）都发生了变化。这时候我们如何保证新的pod和原有的pod的唯一标识不变呢？statefulset可以做到，它能保证pod具有唯一的网络标识。

deployment来管理pod容器的副本数量，一个应用的所有Pod是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，Deployment就可以通过Pod模板创建新的Pod；不需要的时候，Deployment就可以“杀掉”任意一个Pod。但是，在实际的场景中，并不是所有的应用都可以满足这样的要求。尤其是分布式应用，它的多个实例之间，往往有依赖关系，比如：主从关系、主备关系。还有就是数据存储类应用，它的多个实例，往往都会在本地磁盘上保存一份数据。而这些实例一旦被杀掉，即使重建出来，实例与数据之间的对应关系也已经丢失，从而导致应用失败。所以，这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）。

leadchina.yaml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: leadchina
```

consul.yaml

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: consul
  namespace: leadchina
  labels:
    app: consul
    component: server
spec:
  serviceName: consul
  replicas: 3
  selector:
    matchLabels:
      app: consul
      component: server
  template:
    metadata:
      labels:
        app: consul
        component: server
    spec:
      imagePullSecrets:
        - name: harbor
      volumes:
      - name: host-time
        hostPath:
          path: /etc/localtime
      - name: config
        configMap:
          name: consul-config
      containers:
      - name: consul
        image: 10.166.33.110/infra/consul:1.9.2
        imagePullPolicy: IfNotPresent
        args:
          - "agent"
          - "-server"                  # 以server加入集群
          - "-bootstrap-expect=3"      # 组成集群预期需要的数量
          - "-ui"
          - "-config-dir=/etc/consul/config"  # 配置文件目录，所有以.json结尾的文件都会被加载，可以是服务或consul自身的配置
          - "-advertise=$(PODIP)"      # 节点地址
          - "-retry-join=consul-0.consul.$(NAMESPACE).svc.cluster.local"   # 对已知地址情况下，启动时加入的另一位代理的地址
          - "-retry-join=consul-1.consul.$(NAMESPACE).svc.cluster.local"
          - "-retry-join=consul-2.consul.$(NAMESPACE).svc.cluster.local"
        volumeMounts:
          - name: consul
            mountPath: /consul/data
          - name: host-time
            mountPath: /etc/localtime
          - name: config
            mountPath: /etc/consul/config
        env:
          - name: PODIP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
          - containerPort: 8500     # HTTP API 及 Web UI
            name: http
          - containerPort: 8300     #  Server RPC，server 用于接受其他 agent 的请求
            name: server
          - containerPort: 8301     # Serf LAN，数据中心内 gossip 交换数据用
            name: serflan
          - containerPort: 8302     # Serf WAN，跨数据中心 gossip 交换数据用
            name: serfwan
          - containerPort: 8400     # CLI RPC，接受命令行的 RPC 调用
            name: cli-port
          - containerPort: 8600     # DNS 服务，可以把它配置到 53 端口来响应 dns 请求
            name: consuldns
  volumeClaimTemplates:
  - metadata:
      name: consul
      namespace: leadchina
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 5Gi
      storageClassName: nfs
```

headless.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: consul
  namespace: leadchina
  labels:
    name: consul
    component: server
spec:
  clusterIP: None
  ports:
    - name: http
      port: 8500
      targetPort: 8500
    - name: server
      port: 8300
      targetPort: 8300
    - name: serflan-tcp
      protocol: "TCP"
      port: 8301
      targetPort: 8301
    - name: serflan-udp
      protocol: "UDP"
      port: 8301
      targetPort: 8301
    - name: serfwan-tcp
      protocol: "TCP"
      port: 8302
      targetPort: 8302
    - name: serfwan-udp
      protocol: "UDP"
      port: 8302
      targetPort: 8302
    - name: cli-port
      port: 8400
      targetPort: 8400
    - name: consuldns
      port: 8600
      targetPort: 8600
  selector:
    app: consul
---
kind: Service
metadata:
  name: consul-web
  namespace: leadchina
  labels:
    name: consul
    component: server
spec:
  ports:
    - name: http
      protocol: TCP
      port: 8500
      targetPort: 8500
    - name: server
      protocol: TCP
      port: 8300
      targetPort: 8300
    - name: serflan-tcp
      protocol: TCP
      port: 8301
      targetPort: 8301
    - name: serflan-udp
      protocol: UDP
      port: 8301
      targetPort: 8301
    - name: serfwan-tcp
      protocol: TCP
      port: 8302
      targetPort: 8302
    - name: serfwan-udp
      protocol: UDP
      port: 8302
      targetPort: 8302
    - name: cli-port
      port: 8400
      targetPort: 8400
    - name: consuldns
      protocol: TCP
      port: 8600
      targetPort: 8600
  selector:
    app: consul
  type: ClusterIP
```

config.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: consul-config
  namespace: leadchina
data:
  server.json: |
    {
      "bind_addr": "0.0.0.0",           // 应为内部集群通信绑定的地址
      "client_addr": "0.0.0.0",         // consul绑定客户端接口的地址
      "disable_host_node_id": true,     // 将此设置为true将阻止Consul使用来自主机的信息生成确定性节点标识，并将生成随机节点标识，该标识将保留在数据目录中
      "data_dir": "/consul/data",       // consul持久化数据存储位置
      "datacenter": "wuxi_lead",         // 数据中心名称
      "bootstrap_expect": 3,            // 组成集群预期需要的数量
      "server": true,                   // 表示当前使用的server模式
      "domain": "cluster.consul",       // 默认情况下，Consul响应"consul"中的DNS查询
      "retry_join": [                   // k8s集群
        "provider=k8s namespace=demo label_selector=\"app=consul,component=server\""
      ],
      "telemetry": {
        "prometheus_retention_time": "5m"
      }
    }
  ui.json: |
    {
      "ui" : true,                      // 启用内置的Web UI服务器和所需的HTTP路由
      "client_addr" : "0.0.0.0",
      "enable_script_checks" : false,
      "disable_remote_exec" : true
    }
```

应用资源配置清单

```yaml
kubectl apply -f leadchina.yaml
kubectl apply -f headless.yaml
kubectl apply -f config.yaml
kubectl apply -f consul.yaml
# 查看
kubectl get svc -n leadchina
kubectl get pod -n leadchina
kubectl exec -n leadchina consul-0 -- consul members
```



## dotnet服务部署

dotnet项目：

```c#
[Route("")]
public class TestController: Controller
{
    public string Get()
    {
        var hostName = Dns.GetHostName();
        var hostIP = Dns.GetHostEntry(hostName).AddressList.FirstOfDefault(
            x => x.AddressFamily == AddressFamily.InterNetwork).ToString();
        return $"Hello k8s: {DateTime.Now.ToString()}\r\nhostName: {hostName}\r\nhostIP: {hostIP}";
    }
}
```

**部署：**

1、编写Dockerfile

```dockerfile
#指定基础镜像
FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base

#配置工作目录 相当于cd
WORKDIR /app
  
#暴露容器端口，此端口与程序运行路径一致，可
EXPOSE 5000

#复制文件到工作目录
COPY . .
 
#ENV ：配置系统环境变量，比如程序环境环境等在这里配置（开发、预发、线上环境）
#这里是配置程序运行端口，如果程序不使用默认的80端口这里一定要设置（程序运行端口）
ENV ASPNETCORE_URLS http://+:5000

#设置时间为中国上海，默认为UTC时间
ENV TZ=Asia/Shanghai
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

#启动程序
ENTRYPOINT ["dotnet", "AspNetCoreDeployInK8S.dll"]
```

2、制作镜像

```sh
docker build -t easyboys/k8sdemo:coreapi-v1 .
```

3、编写发布应用的deploy.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-demo-deployment
  labels:
    k8s-app: k8s-demo-web
  namespace: leadchina
spec:
  replicas: 2  # 两个副本
  selector:
    matchLabels:
      k8s-app: k8s-demo-web
  template:
    metadata:
      labels:
        k8s-app: k8s-demo-web
    spec:
      containers:
      - name: k8s-demo
        image: easyboys/k8sdemo:coreapi-v1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: k8s-demo-web
  name: k8s-demo-service
  namespace: leadchina
spec:
  type: NodePort
  ports:
  - port: 5000
    targetPort: 5000
  selector:
    k8s-app: k8s-demo-web
```

4、创建命名空间

```sh
kubectl create namespace leadchina
# 查看
kubectl get namespace leadchina
```

5、启动应用

```sh
# 创建deployment：
kubectl create -f deploy.yaml
# 查看
kubectl get svc -n leadchina
# 查看pod，svc状态（对外端口）：
kubectl get svc,pod -o wide
```

5、浏览器访问查看结果



参考：

- https://www.cnblogs.com/roluodev/p/13824191.html
- https://www.cnblogs.com/tylerzhou/p/11100649.html
- https://www.cnblogs.com/justmine/p/8638314.html






## k8s常用命令

```sh
# 设置默认的StorageClass
kubectl patch storageclass managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'


# 查看节点信息（错误信息提示）
kubectl describe pod <podName>

# 查看所有pods
kubectl get pods
# 删除
kubectl delete pods <podName>
# 强制删除
kubectl delete pod PODNAME -n NAMESPACE --grace-period=0 --force
# 发现所有pod会自己起来，这是 repolic 的机制很正确。这是确保 replicas 数量的动作。

# 使用 deployments 命令
kubectl get deployments
kubectl delete deployment <depoyName>
# 删除后 pods 将不在存在
# 查看除了 pod 的资源 
kubectl get rc,service
# 删除所有rc服务
kubectl delete rc --all

# List all pods in ps output format.
kubectl get pods
 
# List all pods in ps output format with more information (such as node name).
kubectl get pods -o wide
 
# List a single replication controller with specified NAME in ps output format.
kubectl get replicationcontroller web
 
# List a single pod in JSON output format.
kubectl get -o json pod web-pod-13je7
 
# List a pod identified by type and name specified in "pod.yaml" in JSON output format.
kubectl get -f pod.yaml -o json
 
# Return only the phase value of the specified pod.
kubectl get -o template pod/web-pod-13je7 –template={{.status.phase}}
 
# List all replication controllers and services together in ps output format.
kubectl get rc,services
 
# List one or more resources by their type and names.
kubectl get rc/web service/frontend pods/web-pod-13je7
```



## Helm

## 简介

什么是 Helm？ 

Helm 为团队提供了在 Kubernetes 内部创建、安装和管理应用程序时需要协作的工具，有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。

有了 Helm，开发者可以：

- 查找要安装和使用的预打包软件（Chart）
- 轻松创建和托管自己的软件包
- 将软件包安装到任何 K8s 集群中
- 查询集群以查看已安装和正在运行的程序包
- 更新、删除、回滚或查看已安装软件包的历史记录

Helm 组件和相关术语

**helm**

- Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。

**Chart**

- Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。

**Repoistory**

- Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。

**Release**

- 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。可以理解为 Helm 使用 Chart 包部署的一个应用实例。



## 安装

地址 https://helm.sh/docs/intro/install/

```sh
wget https://get.helm.sh/helm-v3.3.4-linux-amd64.tar.gz 
tart -zxvf helm-v3.3.4-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/helm

# 查看版本 
helm version

# 命令补全
vim ~/.bashrc
# --------------------------------------------------------
source <(helm completion bash)
# --------------------------------------------------------

source ~/.bashrc
```



## 使用

1、添加常用仓库

```sh
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update # Make sure we get the latest list of charts
helm repo add ali-stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts  #阿里云
# 查看仓库列表
helm repo list
```

2、安装一个mysql的chart

```sh
helm install stable/mysql --generate-name
```

我们需要创建一个pvc，挂载到mysql这个pod中，才能起来mysql。创建storageClass：mysql-sc.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: mysql-sc
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # Ceph pool into which the RBD image shall be created
    pool: replicapool
 
    # RBD image format. Defaults to "2".
    imageFormat: "2"
 
    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering
 
    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
 
    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4
 
# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
```

创建pvc：mysql-pvc.yaml

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-1604294571
spec:
  storageClassName: mysql-sc
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```

查看

```sh
# 查看mysql启动情况
kubectl get po
# 查看使用helm安装的Release（有namespace区分）
helm list
```

卸载

```sh
helm uninstall mysql-1604294571
```



参考：

- https://www.cnblogs.com/bigberg/p/13926052.html



## CICD

## 准备工作

流程图：

![x](C:\Resources\k8s004.png)

![x](C:\Resources\k8s005.png)

服务器环境：

| IP          | 角色                     |
| ----------- | ------------------------ |
| 192.168.0.1 | master1、Harbor、Jenkins |
| 192.168.0.2 | node1                    |
| 192.168.0.3 | node2、Gitlab            |



## 部署Harbor仓库

1、下载harbor二进制文件：https://github.com/goharbor/harbor/releases 

2、安装 docker compose

```bash
sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
# 设置可执行权限
chmod +x /usr/local/bin/docker-compose
```

3、此处应该设置自签证书的，即访问的时候是采用HTTPS进行访问的。此处略去，不影响我们接下去的部署。（后期会出一篇关于自签证书的文章，仅供参考）

4、将下载好的Harbor二进制包上传到服务器上面，然后解压出来

```sh
tar xzvf 包名
```

5、进入解压出来的文件夹harbor中，修改配置文件

```sh
vim harbor.cfg
# 把其中的hostname修改为：master1的IP地址。
# 然后修改harbor的登录密码：harbor_admin_password
```

6、在当前文件夹中开启harbor

```sh
./prepare
./install.sh  # 运行此处的时候需要一定的时间，请等待吧
```

7、启动成功，查看一下（完美的运行）

```sh
docker-compose ps
```

此处介绍一下我创建的项目：

- coresdk：主要用来存放 asp.net core 所需要的sdk
- ops：主要是用来存放jenkins镜像和jenkins-slave镜像
- project：主要存放 asp.netcore 项目的镜像，供k8s拉取

至此，harbor部署完成。



## jenkins-slave镜像搭建

操作服务器：node1

> 说明：jenkins-slave 主要是为了分担jenkins-master的压力。如下图所示：（在多任务运行的时候可以采用）

![x](C:\Resources\k8s006.png)

1、为了创建jenkins-slave镜像，我这边准备了三个文件

- Dockerfile：构建jenkins-slave镜像

- jenkins-slave：shell脚本（需要加入可执行权限 chmod +x jenkins-slave），在镜像构建时需要用到。

- slave.jar: 启动脚本

Dockerfile 文件内容如下：

```dockerfile
FROM ubuntu

ENV JAVA_HOME /user/local/jdk
ENV PATH=${JAVA_HOME}/bin:/usr/local/maven/bin:$PATH

RUN apt-get update && \
    apt-get install -y curl git libltdl-dev && \
    apt-get clean all && \
    rm -rf /var/lib/apt && \
    mkdir -p /usr/share/jenkins
  
COPY slave.jar /usr/share/jenkins/slave.jar
COPY jenkins-slave /usr/bin/jenkins-slave

ENTRYPOINT ["jenkins-slave"]
```

2、配置java的基础环境

配置 JDK和maven，把下载的二进制包放到如下目录：

- apache-maven-3.5.3-bin.tar.gz 解压到的地址为 /usr/local/maven 里面
- jdk-8u45-linux-x64.tar.gz 解压到的地址为 /usr/local/jdk 里面

3、准备好环境后，构建镜像

```sh
docker build -t 192.168.0.1/ops/jenkins-slave .
# 构建完成后把镜像推送到Harbor仓库中
# 在node1中添加harbor仓库的IP地址（否则无法登录）
vim /etc/docker/daemon.json
# ---------------------------------------------------
{
  # ...
  "insecure-registries": ["192.168.0.1"]
}
# ---------------------------------------------------

# 执行 docker info，看一下IP地址是否生效
# 登录
docker login 192.168.0.1 
# 推送
docker push 192.168.0.1/ops/jenkins-slave
```

至此，jenkins-slave 镜像已推送完毕。



## 部署jenkins

部署jenkins时，把jenkins的数据卷采用pv/pvc、nfs 进行挂载。

1、准备文件

- jenkins-service-account.yml：jenkins的服务账号创建

- jenkins.yml：创建容器和service服务。进行可以访问。

- Dockerfile：主要用来生成 jenkins 镜像。

- registry-pull-secret.yaml：部署时可以直接登录harbor仓库拉取镜像（部署jenkins需要用到）

2、切换到node1服务器

```sh
# 把之前准备好的Dockerfile文件构建一下
docker build -t 192.168.0.2/ops/jenkins:lts-alpine .
# 推送到Harbor仓库，命令：
docker push 192.168.0.1/ops/jenkins:lts-alpine
```

3、切换到master服务器

构建 jenkins-service-account.yml 和 jenkins.yml 文件以及 registry-pull-secret.yaml

> 特别注意 registry-pull-secret.yaml 中的 namespace 需要在k8s中创建，data里面的64位认证信息需要登录harbor仓库后将生成的信息黏贴在这里
>
> 生成方式为：在有登录 harbor 的 node 服务器上面执行以下命令：

```sh
cat ~/.docker/config.json |base64 -w0
```

![x](C:\Resources\k8s007.png)

```sh
# 修改上面的文件后，命令：
kubectl create -f 文件名
# 查看jenkins是否运行（还可以看到运行的节点）
kubectl get pod -o wide
# 查看service，记住jenkins的外部访问端口
kubectl get svc
```

4、用浏览器访问：http://运行节点url:端口

1. 在第一次登录的时候需要密码进行解锁jenkins，按照提示获取密码即可
2. 然后“选择插件来安装”，如果需要特别的插件直接选择，否则直接安装即可

5、要想把jenkins关联到k8s中需要安装几个插件

打开“系统管理” => “管理插件” 然后安装

Kubernetes Continuous Deploy、Kubernetes、Gitlab Hook 、GitLab、Build Authorization Token

6、安装完毕后就开始构建项目

![x](C:\Resources\k8s008.png)

7、创建完任务后先不进行配置，我们先要设置一下 jenkins 挂钩 k8s 的环境

点击 “系统管理” => “系统设置”。一直往下滑动，点击“新增一个云”，选择k8s，如果这边没有出现k8s，则代表你的插件没有安装成功，请重新安装吧。

然后配置一下里面的内容，只要配置如下两个地方就可以了。针对URL，我这边是采用 kube-dns 来做服务发现，不需要实际的 ip 地址进行输入。至此搞定。不过还得配置一下 “凭证”，即SSH密钥，方便可以从 gitlab 那边拉取代码，有玩过 gitlab 应该知道，拉取代码分为 git 和 http。

![x](C:\Resources\k8s009.png)

此处如果发现测试不通过，请现在 k8s 中安装一下 kube-dns.yaml

```sh
kubectl create -f kube-dns.yaml
```

8、添加凭证

在这里我添加两个凭证：ssh和k8s的凭证。这个自行添加一下即可。

> 这里要注意一下：凭证生成后，进入凭证里面会有一个自动生成的 ID，此 ID 需要在 asp.net core 项目中的 Jenkinsfile 里面配置。

root 中的密钥是私钥，并且在服务器上需要把公钥配置到 gitlab 上。

![x](C:\Resources\k8s010.png)

这下面是 gitlab 上面配置ssh，ssh的私钥和公钥，直接在 node服务器上生成一下即可 ssh-keygen，然后把里面的内容复制出来即可。

![x](C:\Resources\k8s011.png)



## 部署gitlab

### Centos7

```sh
sudo yum install -y curl policycoreutils-python openssh-server openssh-clients perl
sudo systemctl enable sshd
sudo systemctl start sshd
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo systemctl reload firewalld

sudo yum install postfix
sudo systemctl enable postfix
sudo systemctl start postfix

curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash

sudo EXTERNAL_URL="https://gitlab.example.com" yum install -y gitlab-ce
```

### Kubernetes

1、下载gitlab-ce包

```sh
helm search repo gitlab
helm fetch ali-stable/gitlab-ce
```

2、修改chats包里面的values值

> 这里新建一个gitlab.yaml，然后把下面的信息粘贴进去，注意需要修改storageclass

```yaml
image: gitlab/gitlab-ce:9.4.1-ce.0
externalUrl: http://gitlab.glodon.com
gitlabRootPassword: "Qwerty123456"
serviceType: NodePort
ingress:
  annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
  enabled: true
  tls:
      # - secretName: gitlab.cluster.local
      #   hosts:
      #     - gitlab.cluster.local
  url: gitlab.xxxx.com
sshPort: 22
httpPort: 80
httpsPort: 443
livenessPort: http
readinessPort: http
resources:
  ## GitLab requires a good deal of resources. We have split out Postgres and
  ## redis, which helps some. Refer to the guidelines for larger installs.
  ## ref: https://docs.gitlab.com/ce/install/requirements.html#hardware-requirements
  requests:
    memory: 1Gi
    cpu: 500m
  limits:
    memory: 2Gi
    cpu: 1
persistence:
  ## This volume persists generated configuration files, keys, and certs.
  ##
  gitlabEtc:
    enabled: true
    size: 1Gi
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    ##
    storageClass: prod-sc
    accessMode: ReadWriteOnce
  ## This volume is used to store git data and other project files.
  ## ref: https://docs.gitlab.com/omnibus/settings/configuration.html#storing-git-data-in-an-alternative-directory
  ##
  gitlabData:
    enabled: true
    size: 10Gi
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    ##
    storageClass: "prod-sc"
    accessMode: ReadWriteOnce
postgresql:
  # 9.6 is the newest supported version for the GitLab container
  imageTag: "9.6"
  cpu: 1000m
  memory: 1Gi
  postgresUser: gitlab
  postgresPassword: gitlab
  postgresDatabase: gitlab
  persistence:
    size: 10Gi
    storageClass: "prod-sc"
redis:
  redisPassword: "gitlab"
  resources:
    requests:
      memory: 1Gi
  persistence:
    size: 10Gi
    storageClass: "prod-sc"
```

3、启动gitlab

```sh
helm install --name gitlab  -f gitlab.yaml ./ 
```



## jenkins+gitlab挂钩

上面已经创建了一个任务，然后我们开始配置这个任务里面的内容并且与gitlab挂钩。

1、进入testproject里面配置

按照以下图配置，然后点击保存，这样就完成了任务配置。

![x](C:\Resources\k8s012.png)

接下来就是要配置一下gitlab

![x](C:\Resources\k8s013.png)

![x](C:\Resources\k8s014.png)

2、配置gitlab

在gitlab中创建一个项目Testproject，然后进入到项目中，点击 “Settings” => Integrations，把上面URL和Token复制到这边，然后点击保存即可。

![x](C:\Resources\k8s015.png)

接下来就是测试一下这个配置是否可以用







参考：

- https://www.cnblogs.com/guolianyu/p/9520046.html
- https://blog.csdn.net/walkon1007/article/details/112769224